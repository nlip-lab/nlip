<!DOCTYPE html>

<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Publications - IIT Hyderabad</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,600">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">

  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <!-- <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous"> -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <script src="//code.jquery.com/jquery-1.12.0.min.js"></script>
  <link href="/nlip/css/zoom.css" rel="stylesheet">
  <script src="/nlip/js/zoom.js"></script>
  <script src="/nlip/js/transition.js"></script>
  
  <link rel="stylesheet" href="/nlip/style.css">
  <!-- favicon -->
  <link rel="shortcut icon" href="/nlip/favicon.png">
  <link rel="shortcut icon" href="/nlip/favicon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.11.3/viewer.min.css" integrity="sha512-zdX1vpRJc7+VHCUJcExqoI7yuYbSFAbSWxscAoLF0KoUPvMSAK09BaOZ47UFdP4ABSXpevKfcD0MTVxvh0jLHQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <!-- open graph -->

  
  
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
  <meta property="og:url" content="http://localhost:4000/publication/">
  <meta property="og:title" content="Publications">
  <meta property="og:site_name" content="NLIP: Natural Language and Information Processing Lab">
  <meta property="og:description" content="NLIP Lab is a young and dynamic research group as part of the Department of Computer Science and Engineering at Indian Institute of Technology Hyderabad. We are committed to pushing the frontiers of Natural Language Processing (NLP), Information Retrieval (IR), and related domains through cutting-edge research and development. We collaborate closely with academic institutions, industry partners, and other research groups to drive innovation in these fields. Our focus is on developing efficient and effective algorithms, models, and systems that can scale to real-world applications. We also strive to make our research accessible and impactful by publishing our findings in top-tier conferences and journals, and by open-sourcing our software and datasets." />
  <meta property="og:image" content="/nlip/images/logo/lab_logo.png">
  <style>
    .carousel {
        border-radius: 10px 10px 10px 10px;
        overflow: hidden;
    }
    .grid-container {
    display: grid;
    gap: 5px;
    grid-template-columns: auto auto auto auto;
    padding: 10px;
    }
    .grid-item {
      padding: 10px;
      text-align: center;
    }
    /* footer.sticky-bottom {
      text-align: center;
      font-size: 0.6rem;
      background-color: rgb(228, 228, 228);
      margin:0px;
      padding:30px;
    } */
    footer {
      text-align: center;
      font-size: 0.7rem;
      background-color: rgb(228, 228, 228);
      margin:0px;
      padding:30px;
    }
  </style>
</head>

<body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-light">
  <div class="container">
    <a class="navbar-brand" href="/nlip/" style="font-family:'Open Sans'; text-transform: none;">
      <img src="/nlip/images/logo/lab_logo_no_bg.png" height="80" width="80" class="d-inline-block">NLIP Lab
    </a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mx-auto mb-2 mb-lg-0" style="padding-right: 15px;">
        

        

        <li class="nav-item">
          <a class="nav-link" href="/nlip/">Home</a>
        </li>

        

        

        

        <li class="nav-item">
          <a class="nav-link" href="/nlip/research">Research</a>
        </li>

        

        

        

        <li class="nav-item">
          <a class="nav-link" href="/nlip/publication">Publications</a>
        </li>

        

        

        

        <li class="nav-item">
          <a class="nav-link" href="/nlip/people">People</a>
        </li>

        

        

        

        <li class="nav-item">
          <a class="nav-link" href="/nlip/reading-group">Reading Group</a>
        </li>

        

        

        

        <li class="nav-item">
          <a class="nav-link" href="/nlip/gallery">Gallery</a>
        </li>

        

        

        

        <li class="nav-item">
          <a class="nav-link" href="/nlip/contact">Contact</a>
        </li>

        

        
      </ul>
    </div>
  </div>
</nav>
    <div class="container mt-5 content page">
  <!-- <h1 class="page-title">Publications</h1> -->
  <style>

  /* ...existing code... */

/* Responsive fix for publication cards on mobile */
@media (max-width: 768px) {
  .publication-tile-content {
    flex-direction: column;
    align-items: stretch;
  }
  .publication-image-container {
    display: none !important;
  }
  .publication-content {
    padding: 1rem 0.5rem;
  }
  .year-nav {
    padding: 0.5rem 0;
    margin-bottom: 1rem;
    border-radius: 0;
    box-shadow: none;
    border-bottom: 1px solid #eee;
    position: sticky;
    top: 0;
    background: white;
    z-index: 100;
  }
  .year-nav-container {
    gap: 0.2rem;
    padding: 0 0.2rem;
    overflow-x: auto;
    -webkit-overflow-scrolling: touch;
    scrollbar-width: none;
    -ms-overflow-style: none;
    max-width: 100vw;
    flex-wrap: nowrap;
    white-space: nowrap;
    touch-action: pan-x;
    display: block !important; /* Override flex for mobile */
    justify-content: unset !important;
  }
  .year-nav-container::-webkit-scrollbar {
    display: none;
  }
  .year-nav-item {
    display: inline-block !important; /* Make items inline for scroll */
    padding: 0.6rem 1.2rem;
    font-size: 1rem;
    border-radius: 16px;
    min-width: 60px;
    text-align: center;
    margin: 0 0.1rem;
    white-space: nowrap;
    flex: none;
  }
}

.publication-container {
  width: 100%;
  padding: 1rem 0.5rem;
  max-width: 100%;
  margin: 0;
}

.publication-tile {
  background: white;
  border-radius: 8px;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
  overflow: hidden;
  transition: all 0.3s ease;
  width: 100%;
  margin-bottom: 2rem;
  cursor: pointer;
}

.publication-tile:hover {
  transform: translateY(-5px);
  box-shadow: 0 4px 8px rgba(0,0,0,0.15);
}

.publication-tile.expanded {
  transform: scale(1.02);
  box-shadow: 0 8px 16px rgba(0,0,0,0.2);
}

.publication-tile-content {
  display: flex;
  flex-direction: row;
  align-items: stretch;
  max-width: 100%;
  margin: 0;
}

.publication-image-container {
  width: 400px;
  height: 400px;
  flex: 0 0 400px;
  overflow: hidden;
  display: flex;
  align-items: center;
  justify-content: center;
  align-self: center;
  /* background: #f8f9fa; */
  border-radius: 8px;
}

.publication-image {
  width: 100%;
  height: 100%;
  margin: 0;
  min-width: 0;
  min-height: 0;
  max-width: 100%;
  max-height: 100%;
  object-fit: contain; /* changed from cover to contain */
  display: block;
}

.publication-content {
  flex: 1;
  padding: 1.5rem 1rem;
  display: flex;
  flex-direction: column;
}

.publication-title {
  font-size: 1.2rem;
  font-weight: 600;
  margin-bottom: 0.5rem;
  color: #2c3e50;
}

.publication-title a {
  color: #2c3e50;
  text-decoration: none;
  transition: all 0.2s ease;
  position: relative;
}

.publication-title a:hover {
  color: #3498db;
}

.publication-title a::after {
  content: '';
  position: absolute;
  width: 100%;
  height: 2px;
  bottom: -2px;
  left: 0;
  background-color: #3498db;
  transform: scaleX(0);
  transform-origin: bottom right;
  transition: transform 0.3s ease;
}

.publication-title a:hover::after {
  transform: scaleX(1);
  transform-origin: bottom left;
}

.publication-authors {
  font-size: 0.9rem;
  color: #666;
  margin-bottom: 0.5rem;
}

.publication-journal {
  font-size: 0.85rem;
  color: #0d6efd;
  margin-bottom: 1rem;
}

.publication-summary {
  font-size: 0.9rem;
  line-height: 1.5;
  color: #444;
  margin-bottom: 1rem;
  display: none;
  cursor: pointer;
  position: relative;
}

.publication-summary.preview {
  display: block;
  max-height: 3em;
  overflow: hidden;
  text-overflow: ellipsis;
  display: -webkit-box;
  -webkit-line-clamp: 2;
  -webkit-box-orient: vertical;
}

.publication-tile.expanded .publication-summary {
  display: block;
  max-height: none;
  -webkit-line-clamp: none;
}

.publication-abstract {
  font-size: 0.9rem;
  line-height: 1.6;
  color: #444;
  margin-bottom: 1rem;
  display: none;
  cursor: pointer;
  position: relative;
}

.publication-abstract.preview {
  display: block;
  max-height: 4.8em;
  overflow: hidden;
  text-overflow: ellipsis;
  display: -webkit-box;
  -webkit-line-clamp: 3;
  -webkit-box-orient: vertical;
}

.publication-tile.expanded .publication-abstract {
  display: block;
  max-height: none;
  -webkit-line-clamp: none;
}

.publication-link {
  display: inline-flex;
  align-items: center;
  gap: 0.5rem;
  padding: 0.5rem 1rem;
  background-color: #2c3e50;
  color: white;
  text-decoration: none;
  border-radius: 4px;
  font-size: 0.85rem;
  transition: all 0.2s ease;
}

/* Center align Read More button content vertically */
.btn, .btn.btn-sm.btn-outline-primary {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  height: 100%;
}

.publication-link:hover {
  background-color: #34495e;
  transform: translateY(-2px);
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.publication-link .icon {
  width: 16px;
  height: 16px;
  stroke: currentColor;
  stroke-width: 2;
  fill: none;
  stroke-linecap: round;
  stroke-linejoin: round;
}

.publication-links {
  display: flex;
  flex-wrap: wrap;
  gap: 0.8rem;
  margin-top: 1rem;
}
/*
.publication-links {
  display: flex;
  gap: 0.8rem;
  flex-wrap: wrap;
  margin-top: auto;
} */

.publication-link {
  display: inline-flex;
  align-items: center;
  gap: 0.5rem;
  padding: 0.5rem 1rem;
  background-color: #2c3e50;
  color: white;
  text-decoration: none;
  border-radius: 4px;
  font-size: 0.85rem;
  transition: all 0.2s ease;
}

.publication-link:hover {
  background-color: #34495e;
  transform: translateY(-2px);
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.publication-link .icon {
  width: 16px;
  height: 16px;
  stroke: currentColor;
  stroke-width: 2;
  fill: none;
  stroke-linecap: round;
  stroke-linejoin: round;
}

.publication-links {
  display: flex;
  flex-wrap: wrap;
  gap: 0.8rem;
  margin-top: 1rem;
}

.year-section {
  margin: 3rem 0 2rem;
}

.year-header {
  background: #2c3e50;
  color: white;
  padding: 1rem 1rem;
  border-radius: 8px;
  margin-bottom: 2rem;
  font-size: 1.5rem;
  font-weight: 600;
  width: 100%;
  text-align: center;
}

.year-content {
  display: flex;
  flex-direction: column;
  gap: 2rem;
  padding: 1rem 0.5rem;
  max-width: 100%;
  margin: 0;
}

.expand-icon {
  position: absolute;
  right: 1rem;
  top: 1rem;
  width: 24px;
  height: 24px;
  transition: transform 0.3s ease;
}

.publication-tile.expanded .expand-icon {
  transform: rotate(180deg);
}

/* Custom scrollbar styling */
.publication-container::-webkit-scrollbar {
  height: 8px;
}

.publication-container::-webkit-scrollbar-track {
  background: #f1f1f1;
  border-radius: 4px;
}

.publication-container::-webkit-scrollbar-thumb {
  background: #888;
  border-radius: 4px;
}

.publication-container::-webkit-scrollbar-thumb:hover {
  background: #555;
}

.year-nav {
  position: sticky;
  top: 0;
  background: white;
  padding: 1rem 0;
  margin-bottom: 2rem;
  border-bottom: 1px solid #eee;
  z-index: 100;
  box-shadow: 0 2px 4px rgba(0,0,0,0.05);
}

.year-nav-container {
  max-width: 100%;
  margin: 0;
  display: flex;
  gap: 0.5rem;
  overflow-x: auto;
  padding: 0 0.5rem;
  scrollbar-width: none; /* Firefox */
  -ms-overflow-style: none; /* IE and Edge */
  justify-content: center;
}

.year-nav-container::-webkit-scrollbar {
  display: none; /* Chrome, Safari, Opera */
}

.year-nav-item {
  padding: 0.5rem 1.5rem;
  background: #f8f9fa;
  border-radius: 20px;
  color: #2c3e50;
  text-decoration: none;
  white-space: nowrap;
  transition: all 0.2s ease;
  font-size: 0.9rem;
  border: 1px solid #e9ecef;
}

.year-nav-item:hover {
  background: #e9ecef;
  color: #2c3e50;
  border-color: #2c3e50;
}

.year-nav-item.active {
  background: #2c3e50;
  color: white;
  border-color: #2c3e50;
  font-weight: 500;
}

/* Go to Top Button */
.go-to-top {
  position: fixed;
  bottom: 2rem;
  right: 2rem;
  background: #2c3e50;
  color: white;
  width: 40px;
  height: 40px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  opacity: 0;
  visibility: hidden;
  transition: all 0.3s ease;
  box-shadow: 0 2px 8px rgba(0,0,0,0.2);
  z-index: 1000;
}

.go-to-top.visible {
  opacity: 1;
  visibility: visible;
}

.go-to-top:hover {
  background: #34495e;
  transform: translateY(-3px);
  box-shadow: 0 4px 12px rgba(0,0,0,0.3);
}

.go-to-top svg {
  width: 20px;
  height: 20px;
  fill: currentColor;
}

/* Add these styles before the closing style tag */
.bibtex-modal {
  display: none;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: rgba(0, 0, 0, 0.5);
  z-index: 1000;
  justify-content: center;
  align-items: center;
}

.bibtex-modal.active {
  display: flex;
}

.bibtex-content {
  background: white;
  padding: 2rem;
  border-radius: 8px;
  max-width: 80%;
  max-height: 80vh;
  overflow-y: auto;
  position: relative;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
}

.bibtex-content pre {
  margin: 0;
  white-space: pre;
  font-family: monospace;
  font-size: 0.9rem;
  line-height: 1.5;
  color: #2c3e50;
  background: #f8f9fa;
  padding: 1rem;
  border-radius: 4px;
  border: 1px solid #e9ecef;
}

.bibtex-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 1rem;
  padding-bottom: 0.5rem;
  border-bottom: 1px solid #e9ecef;
}

.bibtex-title {
  font-size: 1.1rem;
  font-weight: 600;
  color: #2c3e50;
}

.bibtex-actions {
  display: flex;
  gap: 0.5rem;
}

.copy-bibtex {
  padding: 0.4rem 0.8rem;
  background: #2c3e50;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  font-size: 0.85rem;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: all 0.2s ease;
}

.copy-bibtex:hover {
  background: #34495e;
  transform: translateY(-1px);
}

.copy-bibtex svg {
  width: 16px;
  height: 16px;
  fill: currentColor;
}

.close-bibtex {
  width: 24px;
  height: 24px;
  cursor: pointer;
  color: #666;
  transition: color 0.2s;
  display: flex;
  align-items: center;
  justify-content: center;
}

.close-bibtex:hover {
  color: #2c3e50;
}
</style>

<script>

document.addEventListener('DOMContentLoaded', function() {
  document.querySelectorAll('.show-bibtex').forEach(function(link) {
    link.addEventListener('click', function(e) {
      e.preventDefault();
      const url = link.getAttribute('href');
      fetch(url)
        .then(response => response.text())
        .then(data => {
          showBibtexModal(data);
        });
    });
  });

  function showBibtexModal(bibtex) {
    let modal = document.getElementById('bibtex-modal');
    if (!modal) {
      modal = document.createElement('div');
      modal.id = 'bibtex-modal';
      modal.style.position = 'fixed';
      modal.style.top = '0';
      modal.style.left = '0';
      modal.style.width = '100vw';
      modal.style.height = '100vh';
      modal.style.background = 'rgba(0,0,0,0.5)';
      modal.style.display = 'flex';
      modal.style.alignItems = 'center';
      modal.style.justifyContent = 'center';
      modal.style.zIndex = '9999';
      modal.innerHTML = `
        <div style="background:#fff;padding:2rem;max-width:90vw;max-height:80vh;overflow:auto;position:relative;border-radius:8px;">
          <button id="close-bibtex-modal" style="position:absolute;top:10px;right:10px;">Close</button>
          <pre style="white-space:pre-wrap;">${bibtex}</pre>
        </div>
      `;
      document.body.appendChild(modal);
      document.getElementById('close-bibtex-modal').onclick = function() {
        modal.remove();
      };
    }
  }
});

document.addEventListener('DOMContentLoaded', function() {
  const tiles = document.querySelectorAll('.publication-tile');
  tiles.forEach(tile => {
    const summary = tile.querySelector('.publication-summary');
    const abstract = tile.querySelector('.publication-abstract');

    if (summary) {
      summary.classList.add('preview');
      summary.addEventListener('click', function(e) {
        e.stopPropagation();
        tile.classList.toggle('expanded');
      });
    }

    if (abstract) {
      abstract.classList.add('preview');
      abstract.addEventListener('click', function(e) {
        e.stopPropagation();
        tile.classList.toggle('expanded');
      });
    }
  });

  // Year navigation functionality
  const yearNavItems = document.querySelectorAll('.year-nav-item');
  const yearSections = document.querySelectorAll('.year-section');
  
  // Update active state based on scroll position
  function updateActiveYear() {
    const scrollPosition = window.scrollY + 100; // Offset for better trigger point

    yearSections.forEach(section => {
      const year = section.querySelector('.year-header').textContent;
      const sectionTop = section.offsetTop - 100;
      const sectionBottom = sectionTop + section.offsetHeight;

      if (scrollPosition >= sectionTop && scrollPosition < sectionBottom) {
        yearNavItems.forEach(item => {
          item.classList.remove('active');
          if (item.textContent === year) {
            item.classList.add('active');
          }
        });
      }
    });
  }

  // Smooth scroll to year section
  yearNavItems.forEach(item => {
    item.addEventListener('click', function(e) {
      e.preventDefault();
      const targetId = this.getAttribute('href');
      const targetSection = document.querySelector(targetId);
      if (targetSection) {
        targetSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
      }
    });
  });

  // Update active state on scroll
  window.addEventListener('scroll', updateActiveYear);
  // Initial active state
  updateActiveYear();

  // Go to Top functionality
  const goToTopButton = document.createElement('div');
  goToTopButton.className = 'go-to-top';
  goToTopButton.innerHTML = `
    <svg viewBox="0 0 24 24">
      <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6zM5 18v2h14v-2H5z"/>
    </svg>
  `;
  document.body.appendChild(goToTopButton);

  // Show/hide button based on scroll position
  window.addEventListener('scroll', function() {
    if (window.scrollY > 300) {
      goToTopButton.classList.add('visible');
    } else {
      goToTopButton.classList.remove('visible');
    }
  });

  // Smooth scroll to top when clicked
  goToTopButton.addEventListener('click', function() {
    window.scrollTo({
      top: 0,
      behavior: 'smooth'
    });
  });

  // Create BibTeX modal
  const modal = document.createElement('div');
  modal.className = 'bibtex-modal';
  modal.innerHTML = `
    <div class="bibtex-content">
      <div class="bibtex-header">
        <div class="bibtex-title">BibTeX</div>
        <div class="bibtex-actions">
          <button class="copy-bibtex">
            <svg viewBox="0 0 24 24">
              <path d="M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z"/>
            </svg>
            Copy
          </button>
          <div class="close-bibtex">
            <svg viewBox="0 0 24 24">
              <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/>
            </svg>
          </div>
        </div>
      </div>
      <pre></pre>
    </div>
  `;
  document.body.appendChild(modal);

  // Handle BibTeX links
  const links = document.querySelectorAll('.publication-link');
  links.forEach(link => {
    const text = link.textContent.toLowerCase();

    if (text.includes('bibtex')) {
      link.addEventListener('click', async function(e) {
        e.preventDefault();
        e.stopPropagation();
        
        const bibtexUrl = this.getAttribute('href');
        try {
          const response = await fetch(bibtexUrl);
          const bibtexContent = await response.text();
          const preElement = modal.querySelector('pre');
          preElement.textContent = bibtexContent;
          modal.classList.add('active');
        } catch (error) {
          console.error('Error loading BibTeX:', error);
        }
      });
    }
  });

  // Copy BibTeX content
  const copyButton = modal.querySelector('.copy-bibtex');
  copyButton.addEventListener('click', () => {
    const bibtexContent = modal.querySelector('pre').textContent;
    navigator.clipboard.writeText(bibtexContent).then(() => {
      const originalText = copyButton.innerHTML;
      copyButton.innerHTML = `
        <svg viewBox="0 0 24 24">
          <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41L9 16.17z"/>
        </svg>
        Copied!
      `;
      setTimeout(() => {
        copyButton.innerHTML = originalText;
      }, 2000);
    });
  });

  // Close modal when clicking close button or outside
  modal.querySelector('.close-bibtex').addEventListener('click', () => {
    modal.classList.remove('active');
  });

  modal.addEventListener('click', (e) => {
    if (e.target === modal) {
      modal.classList.remove('active');
    }
  });

  // Close modal with Escape key
  document.addEventListener('keydown', (e) => {
    if (e.key === 'Escape' && modal.classList.contains('active')) {
      modal.classList.remove('active');
    }
  });
});
</script>

<div class="year-nav">
  <div class="year-nav-container">
    
    <a href="#year-2024" class="year-nav-item">2024</a>
    
    <a href="#year-2023" class="year-nav-item">2023</a>
    
    <a href="#year-2022" class="year-nav-item">2022</a>
    
    <a href="#year-2021" class="year-nav-item">2021</a>
    
    <a href="#year-2020" class="year-nav-item">2020</a>
    
    <a href="#year-2019" class="year-nav-item">2019</a>
    
    <a href="#year-2018" class="year-nav-item">2018</a>
    
    <a href="#year-2017" class="year-nav-item">2017</a>
    
  </div>
</div>

<!-- <p><i>Jump to the full list of publications <a href="#full-list">here</a></i></p> -->

<h2>Publications by Year</h2>

<div class="year-section" id="year-2024">
    <div class="year-header">2024</div>
    <div class="year-content">
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/bok.png" class="publication-image" alt="BoK: Introducing Bag-of-Keywords Loss for Interpretable Dialogue Response Generation" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    BoK: Introducing Bag-of-Keywords Loss for Interpretable Dialogue Response Generation
                        <!-- <a href="/publications/bok/">BoK: Introducing Bag-of-Keywords Loss for Interpretable Dialogue Response Generation</a> -->
                    </h3>

                    <p class="publication-authors">Suvodip Dey, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">25th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2024) (September 2024)</p>
                    
                    <p class="publication-abstract">The standard language modeling (LM) loss by itself has been shown to be inadequate for effective dialogue modeling. As a result, various training approaches, such as auxiliary loss functions and leveraging human feedback, are being adopted to enrich open-domain dialogue systems. One such auxiliary loss function is Bag-of-Words (BoW) loss, defined as the cross-entropy loss for predicting all the words/tokens of the next utterance. In this work, we propose a novel auxiliary loss named Bag-of-Keywords (BoK) loss to capture the central thought of the response through keyword prediction and leverage it to enhance the generation of meaningful and interpretable responses in open-domain dialogue systems. BoK loss upgrades the BoW loss by predicting only the keywords or critical words/tokens of the next utterance, intending to estimate the core idea rather than the entire response. We incorporate BoK loss in both encoder-decoder (T5) and decoder-only (DialoGPT) architecture and train the models to minimize the weighted sum of BoK and LM (BoK-LM) loss. We perform our experiments on two popular open-domain dialogue datasets, DailyDialog and Persona-Chat. We show that the inclusion of BoK loss improves the dialogue generation of backbone models while also enabling post-hoc interpretability. We also study the effectiveness of BoK-LM loss as a reference-free metric and observe comparable performance to the state-of-the-art metrics on various dialogue evaluation datasets.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2024.sigdial-1.48.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/SuvodipDey/BoK" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2024.sigdial-1.48/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/bok.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/bok.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/bok/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/DAC-SIGIR-2024.png" class="publication-image" alt="DAC: Quantized Optimal Transport Reward-based Reinforcement Learning Approach to Detoxify Query Auto-Completion" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    DAC: Quantized Optimal Transport Reward-based Reinforcement Learning Approach to Detoxify Query Auto-Completion
                        <!-- <a href="/publications/dac/">DAC: Quantized Optimal Transport Reward-based Reinforcement Learning Approach to Detoxify Query Auto-Completion</a> -->
                    </h3>

                    <p class="publication-authors">Aishwarya Maheswaran, Kaushal Kumar Maurya, Manish Gupta, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024) (July 2024)</p>
                    
                    <p class="publication-abstract">Modern Query Auto-Completion (QAC) systems utilize natural language generation (NLG) using large language models (LLM) to achieve remarkable performance. However, these systems are prone to generating biased and toxic completions due to inherent learning biases. Existing detoxification approaches exhibit two key limitations: (1) They primarily focus on mitigating toxicity for grammatically well-formed long sentences but struggle to adapt to the QAC task, where queries are short and structurally different (include spelling errors, do not follow grammatical rules and have relatively flexible word order). (2) These approaches often view detoxification through a binary lens where all text labeled as toxic is undesirable and non-toxic is considered desirable. To address these limitations, we propose DAC, an intuitive and efficient reinforcement learning-based model to detoxify QAC. With DAC, we introduce an additional perspective of considering the third query class of addressable toxicity. These queries can encompass implicit toxicity, subjective toxicity, or non-toxic queries containing toxic words. We incorporate this three-class query behavior perspective into the proposed model through quantized optimal transport to learn distinctions and generate truly non-toxic completions. We evaluate toxicity levels in the generated completions by DAC across two real-world QAC datasets (Bing and AOL) using two classifiers: a publicly available generic classifier (Detoxify) and a search query-specific classifier, which we develop (TClassify). we find that DAC consistently outperforms all existing baselines on the Bing dataset and achieves competitive performance on the AOL dataset for query detoxification. % providing high quality and low toxicity. We make the code and models publicly available.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3626772.3657779" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3626772.3657779" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/dac.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/dac.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/dac/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/DQAC-PAKDD-2024.png" class="publication-image" alt="DQAC: Detoxifying Query Auto-Completion with Adapters" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    DQAC: Detoxifying Query Auto-Completion with Adapters
                        <!-- <a href="/publications/dqac/">DQAC: Detoxifying Query Auto-Completion with Adapters</a> -->
                    </h3>

                    <p class="publication-authors">Aishwarya Maheswaran, Kaushal Kumar Maurya, Manish Gupta, Maunendra Sankar Desarkar.</p>
                    <p class="publication-journal">28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2024) (May 2024)</p>
                    
                    <p class="publication-abstract">Recent Query Auto-completion (QAC) systems  leverage natural language generation or pre-trained language models (PLMs) to demonstrate remarkable performance. However, these systems also suffer from biased and toxic completions. Efforts have been made to address language detoxification within PLMs using controllable text generation (CTG) techniques, involving training with non-toxic data and employing decoding time approaches. As the completions for QAC systems are usually short, these existing CTG methods based on decoding and training are not directly transferable. Towards these concerns, we propose the first public QAC detoxification model, Detoxifying Query Auto-Completion (or DQAC), which utilizes adapters in a CTG framework. DQAC operates on latent representations with no additional overhead. It leverages two adapters for toxic and non-toxic cases. During inference, we fuse these representations in a controlled manner that guides the generation of query completions towards non-toxicity. We evaluate toxicity levels in the generated completions across two real-world datasets using two classifiers: a publicly available (Detoxify) and a search query-specific classifier which we develop (QDetoxify). DQAC consistently outperforms all existing baselines and emerges as a state-of-the-art model providing high quality and low toxicity. We make the code publicly available at https://shorturl.at/zJ024</p>
                    
                    <div class="publication-links">

                        
                        
                        
                        <a href="https://link.springer.com/chapter/10.1007/978-981-97-2266-2_9" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/dqac.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/dqac.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/dqac/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/TICOD-PAKDD-2024.png" class="publication-image" alt="Transformer Based Multitask Learning for Image Captioning and Object Detection" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Transformer Based Multitask Learning for Image Captioning and Object Detection
                        <!-- <a href="/publications/ticod/">Transformer Based Multitask Learning for Image Captioning and Object Detection</a> -->
                    </h3>

                    <p class="publication-authors">Debolena Basak, P. K. Srijith, and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2024) (May 2024)</p>
                    
                    <p class="publication-abstract">In several real-world scenarios like autonomous navigation and mobility, to obtain a better visual understanding of the surroundings, image captioning and object detection play a crucial role. This work introduces a novel multitask learning framework that combines image captioning and object detection intoa joint model. We propose TICOD, Transformer-based Image Captioning andObject Detection model for jointly training both tasks by combining the losses obtained from image captioning and object detection networks. By leveraging joint training, the model benefits from the complementary information shared AQ1between the two tasks, leading to improved performance for image captioning.Our approach utilizes a transformer-based architecture that enables end-to-end AQ2network integration for image captioning and object detection and performs bothtasks jointly. We evaluate the effectiveness of our approach through comprehensive experiments on the MS-COCO dataset. Our model outperforms the baselinesfrom image captioning literature by achieving a 3.65% improvement in BERTScore.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://arxiv.org/pdf/2403.06292v1" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1007/978-981-97-2253-2_21" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/ticod.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/ticod.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/ticod/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/CharSpan-EACL-2024.png" class="publication-image" alt="CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages
                        <!-- <a href="/publications/charspan/">CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages</a> -->
                    </h3>

                    <p class="publication-authors">Kaushal Maurya, Rahul Kejriwal, Maunendra Desarkar, Anoop Kunchukuttan</p>
                    <p class="publication-journal">18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024) (March 2024)</p>
                    
                    <p class="publication-abstract">We address the task of machine translation(MT) from extremely low-resource language(ELRL) to English by leveraging cross-lingualtransfer from closely-related high-resourcelanguage (HRL). The development of an MTsystem for ELRL is challenging because theselanguages typically lack parallel corpora andmonolingual corpora, and their representationsare absent from large multilingual languagemodels. Many ELRLs share lexical similaritieswith some HRLs, which presents a novelmodeling opportunity. However, existingsubword-based neural MT models do notexplicitly harness this lexical similarity, as theyonly implicitly align HRL and ELRL latentembedding space. To overcome this limitation,we propose a novel, CHARSPAN, approachbased on character-span noise augmentationinto the training data of HRL. This serves asa regularization technique, making the modelmore robust to lexical divergences betweenthe HRL and ELRL, thus facilitating effectivecross-lingual transfer. Our method significantlyoutperformed strong baselines in zero-shotsettings on closely related HRL and ELRL pairsfrom three diverse language families, emergingas the state-of-the-art model for ELRLs.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2024.eacl-short.26.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://arxiv.org/abs/2305.05214" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/charspan.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/charspan.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/charspan/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    </div>
</div>

<div class="year-section" id="year-2023">
    <div class="year-header">2023</div>
    <div class="year-content">
    
    
    
    
    
    
    
    
    
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/select-noise-2023.png" class="publication-image" alt="SelectNoise: Unsupervised Noise Injection to Enable Zero-shot Machine Translation for Extremely Low-resource languages" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    SelectNoise: Unsupervised Noise Injection to Enable Zero-shot Machine Translation for Extremely Low-resource languages
                        <!-- <a href="/publications/select-noise/">SelectNoise: Unsupervised Noise Injection to Enable Zero-shot Machine Translation for Extremely Low-resource languages</a> -->
                    </h3>

                    <p class="publication-authors">Maharaj Brahma, Kaushal Kumar Maurya, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal"> (October 2023)</p>
                    
                    <p class="publication-abstract">In this work, we focus on the task of machine translation (MT) from extremely low-resource language (ELRLs) to English. The unavailability of parallel data, lack of representation from large multilingual pre-trained models, and limited monolingual data hinder the development of MT systems for ELRLs. However, many ELRLs often share lexical similarities with high-resource languages (HRLs) due to factors such as dialectical variations, geographical proximity, and language structure. We utilize this property to improve cross-lingual signals from closely related HRL to enable MT for ELRLs. Specifically, we propose a novel unsupervised approach, SelectNoise, based on selective candidate extraction and noise injection to generate noisy HRLs training data. The noise injection acts as a regularizer, and the model trained with noisy data learns to handle lexical variations such as spelling, grammar, and vocabulary changes, leading to improved cross-lingual transfer to ELRLs. The selective candidates are extracted using BPE merge operations and edit operations, and noise injection is performed using greedy, top-p, and top-k sampling strategies. We evaluate the proposed model on 12 ELRLs from the FLORES-200 benchmark in a zero-shot setting across two language families. The proposed model outperformed all the strong baselines, demonstrating its efficacy. It has comparable performance with the supervised noise injection model.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2023.findings-emnlp.109.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/maharajbrahma/selectnoise/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2023.findings-emnlp.109/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/select-noise.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/select-noise.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/select-noise/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Towards Low-resource Language Generation with Limited Supervision
                        <!-- <a href="/publications/limited-supervision-nlg-big-picture-workshop/">Towards Low-resource Language Generation with Limited Supervision</a> -->
                    </h3>

                    <p class="publication-authors">Kaushal Kumar Maurya and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">Proceedings of the Big Picture Workshop, Association for Computational Linguistics (October 2023)</p>
                    
                    <p class="publication-abstract">We present a research narrative aimed at enabling language technology for multiple natural language generation (NLG) tasks in low-resource languages (LRLs). With approximately 7,000 languages spoken globally, many lack the resources required for model training. NLG applications for LRLs present two additional key challenges: (i) The training is more pronounced, and (ii) Zero-shot modeling is a viable research direction for scalability; however, generating zero-shot well-formed text in target LRLs is challenging. Addressing these concerns, this narrative introduces three promising research explorations that serve as a step toward enabling language technology for many LRLs. These approaches make effective use of transfer learning and limited supervision techniques for modeling. Evaluations were conducted mostly in the zero-shot setting, enabling scalability. This research narrative is an ongoing doctoral thesis.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2023.bigpicture-1.7.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://aclanthology.org/2023.bigpicture-1.7/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/limited-supervision-nlg-big-picture-workshop.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/limited-supervision-nlg-big-picture-workshop.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/limited-supervision-nlg-big-picture-workshop/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/Trie_QAC3.png" class="publication-image" alt="Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes
                        <!-- <a href="/publications/trie-nlg/">Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes</a> -->
                    </h3>

                    <p class="publication-authors">Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Manish Gupta, Puneet Agrawal</p>
                    <p class="publication-journal">Journal track at European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2023) (July 2023)</p>
                    
                    <p class="publication-abstract">Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. This motivates us to propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model by augmenting the prefix with rich context comprising of recent session queries and top trie completions. This simple modeling approach overcomes the limitations of trie-based and NLG-based approaches and leads to state-of-the-art performance. We evaluate the Trie-NLG model using two large QAC datasets. On average, our model achieves huge ∼57% and ∼14% boost in MRR over the popular trie-based lookup and the strong BART-based baseline methods, respectively.</p>
                    
                    <div class="publication-links">

                        
                        
                        <a href="https://github.com/kaushal0494/Trie-NLG" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://dl.acm.org/doi/10.1007/s10618-023-00966-0" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/trie-nlg.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/trie-nlg.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/trie-nlg/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    On Text Style Transfer via Style-Aware Masked Language Models
                        <!-- <a href="/publications/text-style-transfer/">On Text Style Transfer via Style-Aware Masked Language Models</a> -->
                    </h3>

                    <p class="publication-authors">Sharan Narasimhan, Pooja Shekar,  Suvodip Dey, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">16th International Natural Language Generation Conference (INLG 2023) (July 2023)</p>
                    
                    <p class="publication-abstract">Text Style Transfer (TST) involves transforming a source sentence with a given style label to an output with another target style meanwhile preserving content and fluency. We look at a fill-in-the-blanks approach (also referred to as prototype editing), where the source sentence is stripped off all style-containing words and filled in with suitable words. This closely resembles a Masked Language Model (MLM) objective, with the added initial step of masking only relevant style words rather than BERT's random masking. We show this simple MLM, trained to reconstruct style-masked sentences back into their original style, can even transfer style by making this MLM Style-Aware. This simply involves appending the source sentence with a target style special token. The Style-Aware MLM (SA-MLM), now also accounts for the direction of style transfer and enables style transfer by simply manipulating these special tokens. To learn this n-word to n-word style reconstruction task, we use a single transformer encoder block with 8 heads, 2 layers and no auto-regressive decoder, making it non-generational. We empirically show that this lightweight encoder trained on a simple reconstruction task compares with elaborately engineered state-of-the-art TST models for even complex styles like Discourse or flow of logic, i.e. Contradiction to Entailment and vice-versa. Additionally, we introduce a more accurate attention-based style-masking step and a novel attention-surplus method to determine the position of masks from any arbitrary attribution model in O(1) time. Finally, we show that the SA-MLM arises naturally by considering a probabilistic framework for style transfer.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2023.inlg-main.25.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/sharan21/Style-Masked-Language-Model" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2023.inlg-main.25/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/text-style-transfer.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/text-style-transfer.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/text-style-transfer/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/dial-m_pic.png" class="publication-image" alt="Dial-M: A Masking-based Framework for Dialogue Evaluation" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Dial-M: A Masking-based Framework for Dialogue Evaluation
                        <!-- <a href="/publications/dial-m/">Dial-M: A Masking-based Framework for Dialogue Evaluation</a> -->
                    </h3>

                    <p class="publication-authors">Suvodip Dey and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">24th Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2023)<br /><br /><i> Nominated for Best Paper Award</i> (July 2023)</p>
                    
                    <p class="publication-abstract">In dialogue systems, automatically evaluating machine-generated responses is critical and challenging. Despite the tremendous progress in dialogue generation research, its evaluation heavily depends on human judgments. The standard word-overlapping based evaluation metrics are ineffective for dialogues. As a result, most of the recently proposed metrics are model-based and reference-free, which learn to score different aspects of a conversation. However, understanding each aspect requires a separate model, which makes them computationally expensive. To this end, we propose Dial-M, a Masking-based reference-free framework for Dialogue evaluation. The main idea is to mask the keywords of the current utterance and predict them, given the dialogue history and various conditions (like knowledge, persona, etc.), thereby making the evaluation framework simple and easily extensible for multiple datasets. Regardless of its simplicity, Dial-M achieves comparable performance to state-of-the-art metrics on several dialogue evaluation datasets. We also discuss the interpretability of our proposed metric along with error analysis.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2023.sigdial-1.7.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/SuvodipDey/Dial-M" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2023.sigdial-1.7" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/dial-m.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/dial-m.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/dial-m/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/cl_hyperhawkes.png" class="publication-image" alt="Time-to-Event Modeling with Hypernetwork based Hawkes Process" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Time-to-Event Modeling with Hypernetwork based Hawkes Process
                        <!-- <a href="/publications/cl_hyperhawkes/">Time-to-Event Modeling with Hypernetwork based Hawkes Process</a> -->
                    </h3>

                    <p class="publication-authors">Manisha Dubey, Srijith P. K., Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD 2023) (August 2023)</p>
                    
                    <p class="publication-abstract">Many real-world applications are associated with collection of events with timestamps, known as time-to-event data. Earthquake occurrences, social networks, and user activity logs can be represented as a sequence of discrete events observed in continuous time. Temporal point process serves as an essential tool for modeling such time-to-event data in continuous time space. Despite having massive amounts of event sequence data from various domains like social media, healthcare etc., real world application of temporal point process faces two major challenges: 1) it is not generalizable to predict events from unseen event sequences in dynamic environment 2) they are not capable of thriving in continually evolving environment with minimal supervision while retaining previously learnt knowledge. To tackle these issues, we propose HyperHawkes, a hypernetwork based temporal point process framework which is capable of modeling time of event occurrence for unseen sequences and consequently, zero-shot learning for time-to-event modeling. We also develop a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting. HyperHawkes augments the temporal point process with zero-shot modeling and continual learning capabilities. We demonstrate the application of the proposed framework through our experiments on real-world datasets. Our results show the efficacy of the proposed approach in terms of predicting future events under zero-shot regime for unseen event sequences. We also show that the proposed model is able to learn the time-to-event sequences continually while retaining information from previous event sequences, mitigating catastrophic forgetting in neural temporal point process.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3580305.3599912" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3580305.3599912" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/cl_hyperhawkes.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/cl_hyperhawkes.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/cl_hyperhawkes/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/divhsk_model_arch.png" class="publication-image" alt="DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection
                        <!-- <a href="/publications/divhsk/">DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection</a> -->
                    </h3>

                    <p class="publication-authors">Venkatesh E, Kaushal Kumar Maurya, Deepak Kumar and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">61st Annual Meeting of the Association for Computational Linguistics (ACL 2023) (July 2023)</p>
                    
                    <p class="publication-abstract">Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article, but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Towards this, we propose a novel model called DIVHSK. It has two components: KEYSELECT for selecting the important keywords, and SEQGEN, for finally generating the multiple diverse headlines. In KEYSELECT, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the mostattentive theme and general keywords from the source article. Then, cluster-specific keyword sets guide the SEQGEN, a pre-trained encoderdecoder model, to generate diverse yet semantically similar headlines. The proposed model consistently outperformed existing literature and our strong baselines and emerged as a stateof-the-art model. Additionally, We have also created a high-quality multi-reference headline dataset from news articles</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2023.findings-acl.118.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/kaushal0494/DivHSK" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2023.findings-acl.118/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/divhsk.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/divhsk.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/divhsk/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/vta-ex.png" class="publication-image" alt="Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention
                        <!-- <a href="/publications/vta/">Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention</a> -->
                    </h3>

                    <p class="publication-authors">Arkadipta De, Maunendra Sankar Desarkar, and Asif Ekbal</p>
                    <p class="publication-journal">Natural Language Processing (Elsevier) (July 2023)</p>
                    
                    <p class="publication-abstract">Natural Language Inference (NLI) has been one of the fundamental tasks in Natural Language Processing (NLP). Recognizing Textual Entailment (RTE) between the two pieces of text is a crucial problem. It adds further challenges when it involves two languages, i.e., in the cross-lingual scenario. In this paper, we propose VisioTextual-Attention (VTA) — an effective visual–textual coattention mechanism for multi-modal crosslingual NLI. Through our research, we show that instead of using only linguistic input features, introducing visual features supporting the textual inputs improves the performance of NLI models if an effective cross-modal attention mechanism is carefully constructed. We perform several experiments on a standard cross-lingual textual entailment dataset in Hindi–English language pairs and show that the addition of visual information to the dataset along with our proposed VisioTextual Attention (VTA) enhances performance and surpasses the current state-of-the-art by 4.5%. Through monolingual experiments, we also show that the proposed VTA mechanism surpasses monolingual state-of-the-art by a margin of 2.89%. We argue that our VTA mechanism is model agnostic and can be used with other deep learning-based architectures for grounded cross-lingual NLI.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://pdf.sciencedirectassets.com/782702/1-s2.0-S2949719123X0003X/1-s2.0-S2949719123000201/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDjiEH5Z0dS%2Fj1i%2BxvE%2Ft5LBmE1ENYhH4ow8kj9e%2BBOaQIgQUSZrR%2B%2Ft8f%2FA6phyddv98lb4hMipL1C8GT8kTp9RxgquwUIk%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDNC91mlZ87jLF70onCqPBbWZwRC18c%2Bwj3hYfc43bQ0AQgKt3mG34VsauFEJniQrZ4T4jUUylOa6WsxfZwc7UO%2B70Fxl6W6Cg5tr2iUdrHXDCPiCrj8yHiPbZKuEbb7bYvY7OW8v9%2FsBfQrkItNPlzoDp6RCqsyvy4Q%2FfQBBYyuNPqiBipK11V76rcFyN1Abm0IKFAaKvWj3XTkTAXMxHPzKvfN9Lr5yLRrG%2BwkBm5HwaWkHTj5XpQq8D2B%2B77IUlcbsoOS5Qh9lCXC%2By0RMsFct1m%2BEBR6QTNsuSA1kC8LenFpUYFbAow6SHg3EkTNlmgHheB4PLmUnsJvTkb%2FQ0eyStGwZuV4aAMlb8pofIdSo4G9%2Fc4aXpdb2RAvfMNp3XQEl%2BDOUSFbdgQBLhwgFXPVxLrQXnVa%2FOnBkajtKw5AQ4%2F8pWAOGa5I76eHyYFYGIMpZXHCs7tT3Nm00HDKqeeORsQCK0YO2YNAtctYs3ydZtxe5LsXGPuOfcHJH6hlhHaPSZy18ppoaLLvjkAL0S7Pafu7rCA3rJ%2FTCnXzBlP6LRc9d%2BdfBhL9elZ5%2FpseQ8VyEdG%2FXzz5Nef6Qfvo1zyfMwon%2FD7MZ13J3IHvawKY3fm9PtEi2XwW2pWKeZTJRgFXO9vX4z2ApRPER88McQuTWtYEh9qnGkmSYiEPyVf3%2B6R57gwGS%2Ft65Tz%2FRRfNh1O4XY1D%2BkoV0tUp6KvLnhdpPy2CRVwqXJFuMvUxpjrJ1wMjSXKnAA4AEGJMUn8BtI5HVh%2FepUpYRThCNcDd4w4V1Reo8ZIFbesi%2FQjg7npuwT%2BBp%2Fe5TcLHbm%2BGnumkGlb6OXMwcgsJKDUXC1UcSLREQik8q8c%2BdU5kcB8UEmWzdA%2BkXZHB%2BmoBua0k246gwprHiwQY6sQHj6OleUpPWYNW58w%2Bbw1GO9wtJM8TisQndk5m79BkBoYHTkk%2FIYeKuct9qMN2fFld2zvZAzQHXgg9whi%2FJUMiId2fuPUCLbjEux4Yas5q4s0j6r9SHWWYQtPQmfrhGCKYFVPcKJHj9b9DEn%2FVRrJlWs2hUpCGu4OZmhVSCOiYo%2BMbCN2JB6XMijERWRAsupxuKc8LNUUrWw4VJwRvH9J1pvnaG6s99m76jhRIsaxPHf%2FE%3D&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20250529T174619Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTY7VDOOTEW%2F20250529%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=eed499eb99fc51ba229e75b5ce220d62173babc73c883628dab6bfbcb44d345b&amp;hash=b02db3f58191bdabed2d0fff57d7660f3b7071a51026fab2446395e7e57e3cb9&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S2949719123000201&amp;tid=spdf-a94ad33c-ca53-4fd4-9dc2-95b4235be9e8&amp;sid=1b380ad850dfe64f006ad0e9fa857c99cb79gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=130f5754575806035507&amp;rr=9477cf7c9f9a9a9b&amp;cc=in" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://www.sciencedirect.com/science/article/pii/S2949719123000201" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/vta.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/vta.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/vta/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    </div>
</div>

<div class="year-section" id="year-2022">
    <div class="year-header">2022</div>
    <div class="year-content">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/gnom.png" class="publication-image" alt="GNoM: Graph Neural Network Enhanced Language Models for Disaster Related Multilingual Text Classification" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    GNoM: Graph Neural Network Enhanced Language Models for Disaster Related Multilingual Text Classification
                        <!-- <a href="/publications/gnom/">GNoM: Graph Neural Network Enhanced Language Models for Disaster Related Multilingual Text Classification</a> -->
                    </h3>

                    <p class="publication-authors">Samujjwal Ghosh, Subhadeep Maji, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">WebSci 2022: 14th ACM Web Science Conference 2022 (June 2022)</p>
                    
                    <p class="publication-abstract">Online social media works as a source of various valuable and actionable information during disasters. These information might be available in multiple languages due to the nature of user generated content. An effective system to automatically identify and categorize these actionable information should be capable to handle multiple languages and under limited supervision. However, existing works mostly focus on English language only with the assumption that sufficient labeled data is available. To overcome these challenges, we propose a multilingual disaster related text classification system which is capable to work undervmonolingual, cross-lingual and multilingual lingual scenarios and under limited supervision. Our end-to-end trainable framework combines the versatility of graph neural networks, by applying over the corpus, with the power of transformer based large language models, over examples, with the help of cross-attention between the two. We evaluate our framework over total nine English, Non-English and monolingual datasets invmonolingual, cross-lingual and multilingual lingual classification scenarios. Our framework outperforms state-of-the-art models in disaster domain and multilingual BERT baseline in terms of Weighted F1 score. We also show the generalizability of the proposed model under limited supervision.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3501247.3531561" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/abs/10.1145/3501247.3531561" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <a href="https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3501247.3531561&amp;file=WS22_S1_65.mp4" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polygon points="23 7 16 12 23 17 23 7"></polygon><rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect></svg>
                            Video
                        </a>
                        
                        <!-- 
                        <a href="/nlip/publications/references/gnom.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/gnom.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/gnom/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/hasoc2021.png" class="publication-image" alt="Hostility Detection in Online Hindi-English Code-Mixed Conversations" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Hostility Detection in Online Hindi-English Code-Mixed Conversations
                        <!-- <a href="/publications/hasoc2021/">Hostility Detection in Online Hindi-English Code-Mixed Conversations</a> -->
                    </h3>

                    <p class="publication-authors">Aditi Bagora, Kamal Shrestha, Kaushal Maurya, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">WebSci 2022: 14th ACM Web Science Conference 2022 (June 2022)</p>
                    
                    <p class="publication-abstract">With the rise in accessibility and popularity of various social media platforms, people have started expressing and communicating their ideas, opinions, and interests online. While these platforms are active sources of entertainment and idea-sharing, they also attract hostile and offensive content equally. Identification of hostile posts is an essential and challenging task. In particular, Hindi-English Code-Mixed online posts of conversational nature (which have a hierarchy of posts, comments, and replies) have escalated the challenges. There are two major challenges: (1) the complex structure of Code-Mixed text and (2) filtering the relevant previous context for a given utterance. To overcome these challenges, in this paper, we propose a novel hierarchical neural network architecture to identify hostile posts/comments/replies in online Hindi-English Code-Mixed conversations. We leverage large multilingual pre-trained (mLPT) models like mBERT, XLMR, and MuRIL. The mLPT models provide a rich representation of code-mix text and hierarchical modeling leads to a natural abstraction and selection of the relevant context. The propose model consistently outperformed all the baselines and emerged as a state-of-the-art performing model. We conducted multiple analyses and ablation studies to prove the robustness of the proposed model.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3501247.3531579" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/AditiBagora/Hasoc2021CodeMix" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3501247.3531579" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <a href="https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3501247.3531579&amp;file=WS22_S7_114.mp4" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polygon points="23 7 16 12 23 17 23 7"></polygon><rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect></svg>
                            Video
                        </a>
                        
                        <!-- 
                        <a href="/nlip/publications/references/hasoc2021.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/hasoc2021.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/hasoc2021/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/eval-dst-performance-new.png" class="publication-image" alt="Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances
                        <!-- <a href="/publications/eval-dst-performance/">Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances</a> -->
                    </h3>

                    <p class="publication-authors">Suvodip Dey, Ramamohan Kummara, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (May 2022)</p>
                    
                    <p class="publication-abstract">Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2022.acl-short.35.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/suvodipdey/fga" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2022.acl-short.35/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/eval-dst-performance.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/eval-dst-performance.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/eval-dst-performance/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/time-event-modeling.png" class="publication-image" alt="Continual Learning for Time-to-Event Modeling" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Continual Learning for Time-to-Event Modeling
                        <!-- <a href="/publications/time-event-modeling/">Continual Learning for Time-to-Event Modeling</a> -->
                    </h3>

                    <p class="publication-authors">Manisha Dubey, PK Srijith, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">Continual Lifelong Learning Workshop at ACML 2022 (October 2022)</p>
                    
                    <p class="publication-abstract">Temporal point process serves as an essential tool for modeling time-to-event data in continuous time space. Despite having massive amounts of event sequence data from various domains like social media, healthcare etc., real world application of temporal point process are not capable of thriving in continually evolving environment with minimal supervision while retaining previously learnt knowledge. To tackle this, we propose HyperHawkes, a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting. We demonstrate the application of the proposed framework through our experiments on two real-world datasets.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://openreview.net/pdf?id=1OHWaKZOub" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://openreview.net/forum?id=1OHWaKZOub" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/time-event-modeling.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/time-event-modeling.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/time-event-modeling/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/supervised-graph-contrastive.png" class="publication-image" alt="Supervised Graph Contrastive Pretraining for Text Classification" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Supervised Graph Contrastive Pretraining for Text Classification
                        <!-- <a href="/publications/supervised-graph-contrastive/">Supervised Graph Contrastive Pretraining for Text Classification</a> -->
                    </h3>

                    <p class="publication-authors">Samujjwal Ghosh, Subhadeep Maji, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">In Proceedings of ACM SAC Conference (SAC 2022) (December 2022)</p>
                    
                    <p class="publication-abstract">Contrastive pretraining techniques for text classification has been largely studied in an unsupervised setting. However, oftentimes labeled data from related tasks which share label semantics with current task is available. We hypothesize that using this labeled data effectively can lead to better generalization on current task. In this paper, we propose a novel way to effectively utilize labeled data from related tasks with a graph based supervised contrastive learning approach. We formulate a token-graph by extrapolating the supervised information from examples to tokens. Our formulation results in an embedding space where tokens with high/low probability of belonging to same class are near/further-away from one another. We also develop detailed theoretical insights which serve as a motivation for our method. In our experiments with 13 datasets, we show our method outperforms pretraining schemes by 2.5% and also example-level contrastive learning based formulation by 1.8% on average. In addition, we show cross-domain effectiveness of our method in a zero-shot setting by 3.91% on average. Lastly, we also demonstrate our method can be used as a noisy teacher in a knowledge distillation setting to significantly improve performance of transformer based models in low labeled data regime by 4.57% on average.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://arxiv.org/pdf/2112.11389.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://arxiv.org/abs/2112.11389" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/supervised-graph-contrastive.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/supervised-graph-contrastive.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/supervised-graph-contrastive/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/hyperhawkes.png" class="publication-image" alt="HyperHawkes: Hypernetwork based Neural Temporal Point Process" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    HyperHawkes: Hypernetwork based Neural Temporal Point Process
                        <!-- <a href="/publications/hyperhawkes/">HyperHawkes: Hypernetwork based Neural Temporal Point Process</a> -->
                    </h3>

                    <p class="publication-authors">Manisha Dubey, PK Srijith, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">arXiv preprint arXiv:2205.02309 (August 2022)</p>
                    
                    <p class="publication-abstract">Temporal point process serves as an essential tool for modeling time-to-event data in continuous time space. Despite having massive amounts of event sequence data from various domains like social media, healthcare etc., real world application of temporal point process faces two major challenges: 1) it is not generalizable to predict events from unseen sequences in dynamic environment 2) they are not capable of thriving in continually evolving environment with minimal supervision while retaining previously learnt knowledge. To tackle these issues, we propose 	extit{HyperHawkes}, a hypernetwork based temporal point process framework which is capable of modeling time of occurrence of events for unseen sequences. Thereby, we solve the problem of zero-shot learning for time-to-event modeling. We also develop a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting. In this way, 	extit{HyperHawkes} augments the temporal point process with zero-shot modeling and continual learning capabilities. We demonstrate the application of the proposed framework through our experiments on two real-world datasets. Our results show the efficacy of the proposed approach in terms of predicting future events under zero-shot regime for unseen event sequences. We also show that the proposed model is able to predict sequences continually while retaining information from previous event sequences, hence mitigating catastrophic forgetting for time-to-event data.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://arxiv.org/pdf/2210.00213.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://arxiv.org/abs/2210.00213" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/hyperhawkes.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/hyperhawkes.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/hyperhawkes/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/graph-contrastive-pretraining.png" class="publication-image" alt="Effective utilization of labeled data from related tasks using graph contrastive pretraining: application to disaster related text classification" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Effective utilization of labeled data from related tasks using graph contrastive pretraining: application to disaster related text classification
                        <!-- <a href="/publications/graph-contrastive-pretraining/">Effective utilization of labeled data from related tasks using graph contrastive pretraining: application to disaster related text classification</a> -->
                    </h3>

                    <p class="publication-authors">Samujjwal Ghosh, Subhadeep Maji, and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">SAC '22: Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing (June 2022)</p>
                    
                    <p class="publication-abstract">Contrastive pretraining techniques for text classification has been largely studied in an unsupervised setting. However, oftentimes labeled data from related past datasets which share label semantics with current task is available. We hypothesize that using this labeled data effectively can lead to better generalization on current task. In this paper, we propose a novel way to effectively utilize labeled data from related tasks with a graph based supervised contrastive learning approach. We formulate a token-graph by extrapolating the supervised information from examples to tokens. Our experiments with 8 disaster datasets show our method outperforms baselines and also example-level contrastive learning based formulation. In addition, we show cross-domain effectiveness of our method in a zero-shot setting.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3477314.3507194" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3477314.3507194" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/graph-contrastive-pretraining.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/graph-contrastive-pretraining.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/graph-contrastive-pretraining/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/unsupervised-text-style-transfer.png" class="publication-image" alt="Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer
                        <!-- <a href="/publications/unsupervised-text-style-transfer/">Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer</a> -->
                    </h3>

                    <p class="publication-authors">Sharan Narasimhan, Suvodip Dey, Maundendra Sankar Desarkar</p>
                    <p class="publication-journal">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (May 2022)</p>
                    
                    <p class="publication-abstract">Recent studies show that auto-encoder based approaches successfully perform language generation, smooth sentence interpolation, and style transfer over unseen attributes using unlabelled datasets in a zero-shot manner. The latent space geometry of such models is organised well enough to perform on datasets where the style is 'coarse-grained' i.e. a small fraction of words alone in a sentence are enough to determine the overall style label. A recent study uses a discrete token-based perturbation approach to map 'similar' sentences ('similar' defined by low Levenshtein distance/ high word overlap) close by in latent space. This definition of 'similarity' does not look into the underlying nuances of the constituent words while mapping latent space neighbourhoods and therefore fails to recognise sentences with different style-based semantics while mapping latent neighbourhoods. We introduce EPAAEs (Embedding Perturbed Adversarial AutoEncoders) which completes this perturbation model, by adding a finely adjustable noise component on the continuous embeddings space. We empirically show that this (a) produces a better organised latent space that clusters stylistically similar sentences together, (b) performs best on a diverse set of text style transfer tasks than similar denoising-inspired baselines, and (c) is capable of fine-grained control of Style Transfer strength. We also extend the text style transfer tasks to NLI datasets and show that these more complex definitions of style are learned best by EPAAE. To the best of our knowledge, extending style transfer to NLI tasks has not been explored before.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2022.naacl-main.34.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/sharan21/EPAAE" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2022.naacl-main.34/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2022.naacl-main.34.mp4" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polygon points="23 7 16 12 23 17 23 7"></polygon><rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect></svg>
                            Video
                        </a>
                        
                        <!-- 
                        <a href="/nlip/publications/references/unsupervised-text-style-transfer.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/unsupervised-text-style-transfer.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/unsupervised-text-style-transfer/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/maurya-desarkar-2022-meta.png" class="publication-image" alt="Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation
                        <!-- <a href="/publications/maurya-desarkar-2022-meta/">Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation</a> -->
                    </h3>

                    <p class="publication-authors">Kaushal Maurya and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">Findings of the Association for Computational Linguistics, ACL 2022 (May 2022)</p>
                    
                    <p class="publication-abstract">Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-X$_{NLG}$) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2022.findings-acl.24.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/kaushal0494/Meta_XNLG" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2022.findings-acl.24" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2022.findings-acl.24.mp4" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polygon points="23 7 16 12 23 17 23 7"></polygon><rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect></svg>
                            Video
                        </a>
                        
                        <!-- 
                        <a href="/nlip/publications/references/maurya-desarkar-2022-meta.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/maurya-desarkar-2022-meta.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/maurya-desarkar-2022-meta/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/unspuervised-domain-gnn.png" class="publication-image" alt="Unsupervised Domain Adaptation With Global and Local Graph Neural Networks Under Limited Supervision and Its Application to Disaster Response" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Unsupervised Domain Adaptation With Global and Local Graph Neural Networks Under Limited Supervision and Its Application to Disaster Response
                        <!-- <a href="/publications/unspuervised-domain-gnn/">Unsupervised Domain Adaptation With Global and Local Graph Neural Networks Under Limited Supervision and Its Application to Disaster Response</a> -->
                    </h3>

                    <p class="publication-authors">Sammujwal Ghosh, Subhadeep Maji and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">IEEE Transactions on Computational Social Systems (Volume: 10, Issue: 2, April 2023) (March 2022)</p>
                    
                    <p class="publication-abstract">Identification and categorization of social media posts generated during disasters are crucial to reduce the suffering of the affected people. However, the lack of labeled data is a significant bottleneck in learning an effective categorization system for a disaster. This motivates us to study the problem as unsupervised domain adaptation (UDA) between a previous disaster with labeled data (source) and a current disaster (target). However, if the amount of labeled data available is limited, it restricts the learning capabilities of the model. To handle this challenge, we use limited labeled data along with abundantly available unlabeled data, generated during a source disaster to propose a novel two-part graph neural network (GNN). The first part extracts domain-agnostic global information by constructing a token-level graph across domains and the second part preserves local instance-level semantics. In our experiments, we show that the proposed method outperforms state-of-the-art techniques by 2.74% weighted F1 score on average on two standard public datasets in the area of disaster management. We also report experimental results for granular actionable multilabel classification datasets in disaster domain for the first time, on which we outperform BERT by 3.00% on average w.r.t. weighted F1. Additionally, we show that our approach can retain performance when minimal labeled data are available.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9744724" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://ieeexplore.ieee.org/document/9744724" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/unspuervised-domain-gnn.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/unspuervised-domain-gnn.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/unspuervised-domain-gnn/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    </div>
</div>

<div class="year-section" id="year-2021">
    <div class="year-header">2021</div>
    <div class="year-content">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/hostility.png" class="publication-image" alt="Coarse and Fine-Grained Hostility Detection in Hindi Posts using Fine Tuned Multilingual Embeddings" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Coarse and Fine-Grained Hostility Detection in Hindi Posts using Fine Tuned Multilingual Embeddings
                        <!-- <a href="/publications/hostility/">Coarse and Fine-Grained Hostility Detection in Hindi Posts using Fine Tuned Multilingual Embeddings</a> -->
                    </h3>

                    <p class="publication-authors">Arkadipta De, Venkatesh E, Kaushal Kumar Maurya, and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">CONSTRAIN 2021 (Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situation) (March 2021)</p>
                    
                    <p class="publication-abstract">Due to the wide adoption of social media platforms like Facebook, Twitter, etc., there is an emerging need of detecting online posts that can go against the community acceptance standards. The hostility detection task has been well explored for resource-rich languages like English, but is unexplored for resource-constrained languages like Hindi due to the unavailability of large suitable data. We view this hostility detection as a multi-label multi-class classification problem. We propose an effective neural network-based technique for hostility detection in Hindi posts. We leverage pre-trained multilingual Bidirectional Encoder Representations of Transformer (mBERT) to obtain the contextual representations of Hindi posts. We have performed extensive experiments including different pre-processing techniques, pre-trained models, neural architectures, hybrid strategies, etc. Our best performing neural classifier model includes One-vs-the-Rest approach where we obtained 92.60%, 81.14%, 69.59%, 75.29% and 73.01% F1 scores for hostile, fake, hate, offensive, and defamation labels respectively. The proposed model (https://​github.​com/​Arko98/​Hostility-Detection-in-Hindi-Constraint-2021) outperformed the existing baseline models and emerged as the state-of-the-art model for detecting hostility in the Hindi posts.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://arxiv.org/pdf/2101.04998.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/Arko98/Hostility-Detection-in-Hindi-Constraint-2021" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://www.springerprofessional.de/en/coarse-and-fine-grained-hostility-detection-in-hindi-posts-using/19047892" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/hostility.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/hostility.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/hostility/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/multi-view-hypergraph.png" class="publication-image" alt="Multi-view Hypergraph Convolution Network for Semantic Annotation in LBSNs" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Multi-view Hypergraph Convolution Network for Semantic Annotation in LBSNs
                        <!-- <a href="/publications/multi-view-hypergraph/">Multi-view Hypergraph Convolution Network for Semantic Annotation in LBSNs</a> -->
                    </h3>

                    <p class="publication-authors">Manisha Dubey, P.K Srijith and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">ASONAM 2021: Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (November 2021)</p>
                    
                    <p class="publication-abstract">Semantic characterization of the Point-of-Interest (POI) plays an important role for modeling location-based social networks and various related applications like POI recommendation, link prediction etc. However, semantic categories are not available for many POIs which makes this characterization difficult. Semantic annotation aims to predict such missing categories of POIs. Existing approaches learn a representation of POIs using graph neural networks to predict semantic categories. However, LBSNs involve complex and higher order mobility dynamics. These higher order relations can be captured effectively by employing hypergraphs. Moreover, visits to POIs can be attributed to various reasons like temporal characteristics, spatial context etc. Hence, we propose a Multi-view Hypergraph Convolution Network (Multi-HGCN) where we learn POI representations by considering multiple hypergraphs across multiple views of the data. We build a comprehensive model to learn the POI representation capturing temporal, spatial and trajectorybased patterns among POIs by employing hypergraphs. We use hypergraph convolution to learn better POI representation by using spectral properties of hypergraph. Experiments conducted on three real-world datasets show that the proposed approach outperforms the state-of-the-art approaches.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3487351.3488341" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3487351.3488341" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/multi-view-hypergraph.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/multi-view-hypergraph.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/multi-view-hypergraph/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/maurya-etal-2021-zmbart.png" class="publication-image" alt="ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation
                        <!-- <a href="/publications/maurya-etal-2021-zmbart/">ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation</a> -->
                    </h3>

                    <p class="publication-authors">Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Yoshinobu Kano, and Kumari Deepshikha</p>
                    <p class="publication-journal">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 (August 2021)</p>
                    
                    <p class="publication-abstract">Despite the recent advancement in NLP research, cross-lingual transfer for natural language generation is relatively understudied. In this work, we transfer supervision from high resource language (HRL) to multiple lowresource languages (LRLs) for natural language generation (NLG). We consider four NLG tasks (text summarization, question generation, news headline generation, and distractor generation) and three syntactically diverse languages, i.e., English, Hindi, and Japanese. We propose an unsupervised crosslingual language generation framework (called ZmBART) that does not use any parallel or pseudo-parallel/back-translated data. In this framework, we further pre-train mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task using monolingual data of three languages. The objective function of the auxiliary task is close to the target tasks which enriches the multi-lingual latent representation of mBART and provides good initialization for target tasks. Then, this model is fine-tuned with task-specific supervised English data and directly evaluated with low-resource languages in the Zero-shot setting. To overcome catastrophic forgetting and spurious correlation issues, we applied freezing model component and data argumentation approaches respectively. This simple modeling approach gave us promising results. We experimented with few-shot training (with 1000 supervised data-points) which boosted the model performance further. We performed several ablations and cross-lingual transferability analysis to demonstrate the robustness of ZmBART.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2021.findings-acl.248.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/kaushal0494/ZmBART" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2021.findings-acl.248/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2021.findings-acl.248.mp4" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polygon points="23 7 16 12 23 17 23 7"></polygon><rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect></svg>
                            Video
                        </a>
                        
                        <!-- 
                        <a href="/nlip/publications/references/maurya-etal-2021-zmbart.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/maurya-etal-2021-zmbart.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/maurya-etal-2021-zmbart/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/hidst.png" class="publication-image" alt="Hi-DST: A Hierarchical Approach for Scalable and Extensible Dialogue State Tracking" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Hi-DST: A Hierarchical Approach for Scalable and Extensible Dialogue State Tracking
                        <!-- <a href="/publications/hidst/">Hi-DST: A Hierarchical Approach for Scalable and Extensible Dialogue State Tracking</a> -->
                    </h3>

                    <p class="publication-authors">Suvodip Dey, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue (July 2021)</p>
                    
                    <p class="publication-abstract">Dialogue State Tracking (DST) is a sub-task of task-based dialogue systems where the user intention is tracked through a set of (domain, slot, slot-value) triplets. Existing DST models can be difficult to extend for new datasets with larger domains/slots mainly due to either of the two reasons- i) prediction of domain-slot as a pair, and ii) dependency of model parameters on the number of slots and domains. In this work, we propose to address these issues using a Hierarchical DST (Hi-DST) model. At a given turn, the model first detects a change in domain followed by domain prediction if required. Then it decides suitable action for each slot in the predicted domains and finds their value accordingly. The model parameters of Hi-DST are independent of the number of domains/slots. Due to the hierarchical modeling, it achieves O(|M|+|N|) belief state prediction for a single turn where M and N are the set of unique domains and slots respectively. We argue that the hierarchical structure helps in the model explainability and makes it easily extensible to new datasets. Experiments on the MultiWOZ dataset show that our proposed model achieves comparable joint accuracy performance to state-of-the-art DST models.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://aclanthology.org/2021.sigdial-1.23.pdf" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/suvodipdey/hi-dst" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://aclanthology.org/2021.sigdial-1.23/" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <a href="https://www.youtube.com/watch?v=ldnP2Cn_7F0" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polygon points="23 7 16 12 23 17 23 7"></polygon><rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect></svg>
                            Video
                        </a>
                        
                        <!-- 
                        <a href="/nlip/publications/references/hidst.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/hidst.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/hidst/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    </div>
</div>

<div class="year-section" id="year-2020">
    <div class="year-header">2020</div>
    <div class="year-content">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/granular-classification-framework.png" class="publication-image" alt="Semi-Supervised Granular Classification Framework for Resource Constrained Short-texts: Towards Retrieving Situational Information During Disaster Events" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Semi-Supervised Granular Classification Framework for Resource Constrained Short-texts: Towards Retrieving Situational Information During Disaster Events
                        <!-- <a href="/publications/granular-classification-framework/">Semi-Supervised Granular Classification Framework for Resource Constrained Short-texts: Towards Retrieving Situational Information During Disaster Events</a> -->
                    </h3>

                    <p class="publication-authors">Samujjwal Ghosh and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">WebSci 2020: 12th ACM Conference on Web Science (July 2020)</p>
                    
                    <p class="publication-abstract">During the time of disasters, lots of short-texts are generated containing crucial situational information. Proper extraction and identification of situational information might be useful for various rescue and relief operations. Few specific types of infrequent situational information might be critical. However, obtaining labels for those resource-constrained classes is challenging as well as expensive. Supervised methods pose limited usability in such scenarios. To overcome this challenge, we propose a semi-supervised learning framework which utilizes abundantly available unlabelled data by self-learning. The proposed framework improves the performance of the classifier for resource-constrained classes by selectively incorporating highly confident samples from unlabelled data for self-learning. Incremental incorporation of unlabelled data, as and when they become available, is suitable for ongoing disaster mitigation. Experiments on three disaster-related datasets show that such improvement results in overall performance increase over standard supervised approach.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3394231.3397892" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3394231.3397892" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <a href="https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3394231.3397892&amp;file=3394231.3397892.mp4" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polygon points="23 7 16 12 23 17 23 7"></polygon><rect x="1" y="5" width="15" height="14" rx="2" ry="2"></rect></svg>
                            Video
                        </a>
                        
                        <!-- 
                        <a href="/nlip/publications/references/granular-classification-framework.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/granular-classification-framework.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/granular-classification-framework/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    A Neural Approach for Detecting Inline Mathematical Expressions from Scientific Documents
                        <!-- <a href="/publications/pericles_1468039438/">A Neural Approach for Detecting Inline Mathematical Expressions from Scientific Documents</a> -->
                    </h3>

                    <p class="publication-authors">Sreekanth Madisetty, Kaushal Kumar Maurya, Akiko Aizawa, and Maunendra Sankar Desarkar,</p>
                    <p class="publication-journal">Wiley Expert Systems (May 2020)</p>
                    
                    <p class="publication-abstract">Scientific documents generally contain multiple mathematical expressions in them. Detecting inline mathematical expressions are one of the most important and challenging tasks in scientific text mining. Recent works that detect inline mathematical expressions in scientific documents have looked at the problem from an image processing perspective. There is little work that has targeted the problem from NLP perspective. Towards this, we define a few features and applied Conditional Random Fields (CRF) to detect inline mathematical expressions in scientific documents. Apart from this feature based approach, we also propose a hybrid algorithm that combines Bidirectional Long Short Term Memory networks (Bi-LSTM) and feature-based approach for this task. Experimental results suggest that this proposed hybrid method outperforms several baselines in the literature and also individual methods in the hybrid approach.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://doi.org/10.1111/exsy.12576" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12576" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/pericles_1468039438.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/pericles_1468039438.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/pericles_1468039438/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/hapsap.png" class="publication-image" alt="HAP-SAP: Semantic Annotation in LBSNs using Latent Spatio-Temporal Hawkes Process" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    HAP-SAP: Semantic Annotation in LBSNs using Latent Spatio-Temporal Hawkes Process
                        <!-- <a href="/publications/hapsap/">HAP-SAP: Semantic Annotation in LBSNs using Latent Spatio-Temporal Hawkes Process</a> -->
                    </h3>

                    <p class="publication-authors">Manisha Dubey, P.K Srijith and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">SIGSPATIAL 2020: Proceedings of the 28th International Conference on Advances in Geographic Information Systems (November 2020)</p>
                    
                    <p class="publication-abstract">The prevalence of location-based social networks (LBSNs) has eased the understanding of human mobility patterns. However, categories which act as semantic characterization of the location, might be missing for some check-ins and can adversely affect modelling the mobility dynamics of users. At the same time, mobility patterns provide cues on the missing semantic categories. In this paper, we simultaneously address the problem of semantic annotation of locations and location adoption dynamics of users. We propose our model HAP-SAP, a latent spatio-temporal multivariate Hawkes process, which considers latent semantic category influences, and temporal and spatial mobility patterns of users. The inferred semantic categories can supplement our model on predicting the next check-in events by users. Our experiments on real datasets demonstrate the effectiveness of the proposed model for the semantic annotation and location adoption modelling tasks.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3397536.3422233" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3397536.3422233" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/hapsap.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/hapsap.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/hapsap/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/learningtodistract.png" class="publication-image" alt="Learning to Distract: A Hierarchical Multi-Decoder Network for Automated Generation of Long Distractors for Multiple-Choice Questions for Reading Comprehension" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Learning to Distract: A Hierarchical Multi-Decoder Network for Automated Generation of Long Distractors for Multiple-Choice Questions for Reading Comprehension
                        <!-- <a href="/publications/learningtodistract/">Learning to Distract: A Hierarchical Multi-Decoder Network for Automated Generation of Long Distractors for Multiple-Choice Questions for Reading Comprehension</a> -->
                    </h3>

                    <p class="publication-authors">Kaushal Kumar Maurya and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">CIKM 2020: Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management (October 2020)</p>
                    
                    <p class="publication-abstract">The task of generating incorrect options for multiple-choice questions is termed as distractor generation problem. The task requires high cognitive skills and is extremely challenging to automate. Existing neural approaches for the task leverage encoder-decoder architecture to generate long distractors. However, in this process two critical points are ignored - firstly, many methods use Jaccard similarity over a pool of candidate distractors to sample the distractors. This often makes the generated distractors too obvious or not relevant to the question context. Secondly, some approaches did not consider the answer in the model, which caused the generated distractors to be either answer-revealing or semantically equivalent to the answer. In this paper, we propose a novel Hierarchical Multi-Decoder Network (HMD-Net) consisting of one encoder and three decoders, where each decoder generates a single distractor. To overcome the first problem mentioned above, we include multiple decoders with a dis-similarity loss in the loss function. To address the second problem, we exploit richer interaction between the article, question, and answer with a SoftSel operation and a Gated Mechanism. This enables the generation of distractors that are in context with questions but semantically not equivalent to the answers. The proposed model outperformed all the previous approaches significantly in both automatic and manual evaluations. In addition, we also consider linguistic features and BERT contextual embedding with our base model which further push the model performance.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3340531.3411997" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        <a href="https://github.com/kaushal0494/HMD_Network" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><polyline points="16 18 22 12 16 6"></polyline><polyline points="8 6 2 12 8 18"></polyline></svg>
                            Code
                        </a>
                        
                        
                        <a href="https://dl.acm.org/doi/abs/10.1145/3340531.3411997" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/learningtodistract.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/learningtodistract.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/learningtodistract/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
    
    
    
    
    
    
    
    
    
    </div>
</div>

<div class="year-section" id="year-2019">
    <div class="year-header">2019</div>
    <div class="year-content">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/multi-context-info.png" class="publication-image" alt="Multi-Context Information for Word Representation Learning" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Multi-Context Information for Word Representation Learning
                        <!-- <a href="/publications/multi-context-info/">Multi-Context Information for Word Representation Learning</a> -->
                    </h3>

                    <p class="publication-authors">Swapnil Dewalkar and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">DocEng 2019: Proceedings of the ACM Symposium on Document Engineering 2019 (September 2019)</p>
                    
                    <p class="publication-abstract">Word embedding techniques in literature are mostly based on Bag of Words models where words that co-occur with each other are considered to be related. However, it is not necessary for similar or related words to occur in the same context window. In this paper, we propose a new approach to combine different types of resources for training word embeddings. The lexical resources used in this work are Dependency Parse Tree and WordNet. Apart from the co-occurrence information, the use of these additional resources helps us in including the semantic and syntactic information from the text in learning the word representations. The learned representations are evaluated on multiple evaluation tasks like Semantic Textual Similarity, Word Similarity. Results of the experimental analyses highlight the usefulness of the proposed methodology.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3342558.3345418" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3342558.3345418" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/multi-context-info.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/multi-context-info.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/multi-context-info/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/vitag.png" class="publication-image" alt="ViTag: Automatic Video Tagging Using Segmentation and Conceptual Inference" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    ViTag: Automatic Video Tagging Using Segmentation and Conceptual Inference
                        <!-- <a href="/publications/vitag/">ViTag: Automatic Video Tagging Using Segmentation and Conceptual Inference</a> -->
                    </h3>

                    <p class="publication-authors">Abhishek A. Patwardhan, Santanu Das, Sakshi Varshney, Maunendra Sankar Desarkar, Debi Prosad Dogra</p>
                    <p class="publication-journal">2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM) (September 2019)</p>
                    
                    <p class="publication-abstract">Massive increase in multimedia data has created a need for effective organization strategy. The multimedia collection is organized based on attributes such as domain, index-terms, content description, owners, etc. Typically, index-term is a prominent attribute for effective video retrieval systems. In this paper, we present a new approach of automatic video tagging referred to as ViTag. Our analysis relies upon various image similarity metrics to automatically extract key-frames. For each key-frame, raw tags are generated by performing reverse image tagging. The final step analyzes raw tags in order to discover hidden semantic information. On a dataset of 103 videos belonging to 13 domains derived from various YouTube categories, we are able to generate tags with 65.51% accuracy. We also rank the generated tags based upon the number of proper nouns present in it. The geometric mean of Reciprocal Rank estimated over the entire collection has been found to be 0.873.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8919469" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://ieeexplore.ieee.org/document/8919469" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/vitag.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/vitag.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/vitag/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
    
    
    
    
    
    </div>
</div>

<div class="year-section" id="year-2018">
    <div class="year-header">2018</div>
    <div class="year-content">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Get me the best: predicting best answerers in community question answering sites
                        <!-- <a href="/publications/recsys18/">Get me the best: predicting best answerers in community question answering sites</a> -->
                    </h3>

                    <p class="publication-authors">Rohan Tondulkar, Manisha Dubey and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">RecSys 2018: Proceedings of the 12th ACM Conference on Recommender Systems (September 2018)</p>
                    
                    <p class="publication-abstract">There has been a massive rise in the use of Community Question and Answering (CQA) forums to get solutions to various technical and non-technical queries. One common problem faced in CQA is the small number of experts, which leaves many questions unanswered. This paper addresses the challenging problem of predicting the best answerer for a new question and thereby recommending the best expert for the same. Although there are work in the literature that aim to find possible answerers for questions posted in CQA, very few algorithms exist for finding the best answerer whose answer will satisfy the information need of the original Poster. For finding answerers, existing approaches mostly use features based on content and tags associated with the questions. There are few approaches that additionally consider the users' history. In this paper, we propose an approach that considers a comprehensive set of features including but not limited to text representation, tag based similarity as well as multiple user-based features that target users' availability, agility as well as expertise for predicting the best answerer for a given question. We also include features that give incentives to users who answer less but more important questions over those who answer a lot of questions of less importance. A learning to rank algorithm is used to find the weight of each feature. Experiments conducted on a real dataset from Stack Exchange show the efficacy of the proposed method in terms of multiple evaluation metrics for accuracy, robustness and real time performance.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3240323.3240346" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3240323.3240346" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/recsys18.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/recsys18.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/recsys18/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-image-container">
                    <img src="/nlip/publications/images/class-specific-tfidf.png" class="publication-image" alt="Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters" />
                </div>
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters
                        <!-- <a href="/publications/class-specific-tfidf/">Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters</a> -->
                    </h3>

                    <p class="publication-authors">Samujjwal Ghosh and Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">WWW 2018: Companion Proceedings of the The Web Conference 2018 (April 2018)</p>
                    
                    <p class="publication-abstract">Proper formulation of features plays an important role in short-text classification tasks as the amount of text available is very little. In literature, Term Frequency - Inverse Document Frequency (TF-IDF) is commonly used to create feature vectors for such tasks. However, TF-IDF formulation does not utilize the class information available in supervised learning. For classification problems, if it is possible to identify terms that can strongly distinguish among classes, then more weight can be given to those terms during feature construction phase. This may result in improved classifier performance with the incorporation of extra class label related information. We propose a supervised feature construction method to classify tweets, based on the actionable information that might be present, posted during different disaster scenarios. Improved classifier performance for such classification tasks can be helpful in the rescue and relief operations. We used three benchmark datasets containing tweets posted during Nepal and Italy earthquakes in 2015 and 2016 respectively. Experimental results show that the proposed method obtains better classification performance on these benchmark datasets.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3184558.3191621" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://dl.acm.org/doi/10.1145/3184558.3191621" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/class-specific-tfidf.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/class-specific-tfidf.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/class-specific-tfidf/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    
    
    </div>
</div>

<div class="year-section" id="year-2017">
    <div class="year-header">2017</div>
    <div class="year-content">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        

        
        <div class="publication-tile">
            <div class="publication-tile-content">
                
                <div class="publication-content">

                    <h3 class="publication-title">
                    Using social media for classifying actionable insights in disaster scenario
                        <!-- <a href="/publications/classifying-actionable-insights/">Using social media for classifying actionable insights in disaster scenario</a> -->
                    </h3>

                    <p class="publication-authors">Samujjwal Ghosh, PK Srijith, Maunendra Sankar Desarkar</p>
                    <p class="publication-journal">International Journal of Advances in Engineering Sciences and Applied Mathematics volume 9, pages224–237 (December 2017)</p>
                    
                    <p class="publication-abstract">Micro-blogging sites are important source of real-time situational information during disasters such as earthquakes, hurricanes, wildfires, flood etc. Such disasters cause miseries in the lives of affected people. Timely identification of steps needed to help the affected people in such situations can mitigate those miseries to a large extent. In this paper, we focus on the problem of automated classification of disaster related tweets to a set of predefined categories. Some example categories considered are resource availability, resource requirement, infrastructure damage etc. Proper annotation of the tweets with these class information can help in timely determination of the steps needed to be taken to address the concerns of the people in the affected areas. Depending on the information category, different feature sets might be useful for proper identification of posts belonging to that category. In this work, we define multiple feature sets and use them with various supervised classification algorithms from literature to study the effectiveness of our approach in annotating the tweets with their appropriate information categories.</p>
                    
                    <div class="publication-links">

                        
                        <a href="https://link.springer.com/article/10.1007/s12572-017-0197-2" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            PDF
                        </a>
                        
                        
                        
                        <a href="https://link.springer.com/article/10.1007/s12572-017-0197-2" class="publication-link" target="_blank">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line></svg>
                            URL
                        </a>
                        
                        
                        <!-- 
                        <a href="/nlip/publications/references/classifying-actionable-insights.txt" class="publication-link show-bibtex">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                         -->
                        
                        <a href="/nlip/publications/references/classifying-actionable-insights.bib" class="publication-link">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                            BibTeX
                        </a>
                        
                        <a href="/nlip/publications/classifying-actionable-insights/" class="btn btn-sm btn-outline-primary" target="_blank" style="font-size: 0.95rem; padding: 0.45rem 1.1rem; min-width: unset; min-height: unset; height: auto; line-height: 1.2; border-radius: 6px; white-space: nowrap; display: inline-flex; align-items: center; justify-content: center;">
                          Read More
                        </a>

                    </div>
                </div>
            </div>
        </div>
        
    
    
    </div>
</div>


</div>

    
<footer class="footer mt-1">
  <div>
    <a href="https://nlip-lab.github.io" style="font-size: 0.8rem;">Natural Language and Information Processing Lab</a><br>
    <a href="https://iith.ac.in" style="color: black;">Indian Institute of Technology Hyderabad (IITH)</a>, Hyderabad, India
  </div>
  
  <div class="mt-3">
    For more details visit our <a href="/nlip/contact">Contact us</a> page
  </div>

  <span style="display: block; margin-bottom: 2em"></span>
  <div class="container">
    <!-- &copy; Copyright 2025 NLIP Lab, All Rights Reserved.
    <br> -->Last updated: June 17, 2025.
    <br>
    
    Website template inspired from <a href="https://kordinglab.com/">Kording Lab</a><br>
  </div>

</footer>

</html> 

    <!-- <script src="http://code.jquery.com/jquery-2.2.1.min.js"></script> -->
    <!-- <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script> -->
    <!-- <script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script> -->
    <!-- <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz" crossorigin="anonymous"></script> -->
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.2/dist/umd/popper.min.js" integrity="sha384-IQsoLXl5PILFhosVNubq5LC7Qb9DXgDA9i+tQ8Zj3iwWAwPtgFTxbJ8NT4GN1R8p" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.min.js" integrity="sha384-cVKIPhGWiC2Al4u+LWgxfKTRIcfu0JTxR+EQDz/bgldoEyl4H0zUF0QKbrJ0EcQF" crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.11.3/viewer.min.js" integrity="sha512-f8kZwYACKF8unHuRV7j/5ILZfflRncxHp1f6y/PKuuRpCVgpORNZMne1jrghNzTVlXabUXIg1iJ5PvhuAaau6Q==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>


    
</body>
</html>
