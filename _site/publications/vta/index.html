<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention - IIT Hyderabad</title>

  <!-- CSS from default.html -->
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,600">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
  <link href="/nlip/css/zoom.css" rel="stylesheet">
  <link rel="stylesheet" href="/nlip/style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.11.3/viewer.min.css"
    integrity="sha512-zdX1vpRJcExqoI7yuYbSFAbSWxscAoLF0KoUPvMSAK09BaOZ47UFdP4ABSXpevKfcD0MTVxvh0jLHQ=="
    crossorigin="anonymous" referrerpolicy="no-referrer" />

  <!-- Favicons: Prioritize spotlight's, then default's -->
  <link rel="icon" type="image/x-icon" href="/nlip/static/images/ns.png">
  <!-- From original spotlight.html -->
  <link rel="shortcut icon" href="/nlip/favicon.png"> <!-- From default.html -->

  <!-- OG Meta Tags: Combine and prioritize page-specific from spotlight -->
  <meta name="description" content="Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention"> <!-- From original spotlight.html -->
  <meta property="og:title" content="Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention" /> <!-- From original spotlight.html -->
  <meta property="og:description" content="Natural Language Inference (NLI) has been one of the fundamental tasks in Natural Language Processing (NLP). Recognizing Textual Entailment (RTE) between the..." />
  <!-- From original spotlight.html -->
  <meta property="og:url" content="nlip-lab.github.io/nlip/publications/vta/" />
  <!-- From original spotlight.html -->
  <meta property="og:image" content="/nlip/publications/images/vta-ex.png" />
  <!-- From original spotlight.html -->
  <meta property="og:image:width" content="1200" /> <!-- From original spotlight.html -->
  <meta property="og:image:height" content="630" /> <!-- From original spotlight.html -->
  <meta property="og:locale" content="en_US"> <!-- From default.html -->
  <meta property="og:type" content="article"> <!-- From default.html -->
  <meta property="og:site_name" content="NLIP: Natural Language and Information Processing Lab"> <!-- From default.html -->

  <!-- CSS from original spotlight.html -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="/nlip/static/css/bulma.min.css">
  <link rel="stylesheet" href="/nlip/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="/nlip/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="/nlip/static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/nlip/static/css/index.css">

  <!-- Scripts for head from default.html -->
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script src="//code.jquery.com/jquery-1.12.0.min.js"></script>
  <script src="/nlip/js/zoom.js"></script>
  <script src="/nlip/js/transition.js"></script>

  <style>
    /* Styles from default.html (general site styles) */
    .carousel {
      border-radius: 10px;
      overflow: hidden;
    }

    .grid-container {
      display: grid;
      gap: 5px;
      grid-template-columns: auto auto auto auto;
      padding: 10px;
    }

    .grid-item {
      padding: 10px;
      text-align: center;
    }

    /* Styles from original spotlight.html (specific to spotlight pages) */
    .image-card {
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
      overflow: hidden;
      transition: transform 0.2s ease-in-out;
    }

    .image-card:hover {
      transform: scale(1.05);
    }

    .image-table {
      margin: 30px auto;
      text-align: center;
    }
  </style>
</head>

<body id="page-top">
  <section class="hero">
    <!-- <div class="hero-body" style="padding-top: 2.5rem;"> Add space above title -->
    <div class="container is-max-desktop" style="padding: 2rem;">
      <div class="columns is-centered">
        <div class="column has-text-centered" style="text-align: center;">
          <!-- Center align everything before abstract -->
          <h1 class="title is-1 publication-title" style="text-align: center;">Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention</h1>
          <div class="is-size-5 publication-authors" style="text-align: center;">
            <p>Arkadipta De, Maunendra Sankar Desarkar, and Asif Ekbal</p>

          </div>
          <div class="is-size-5 publication-authors" style="text-align: center;">
            

            <br><span style="font-size:1.3rem; font-weight:700; color:#2c3e50;">Natural Language Processing (Elsevier)</span>
          </div>
          <div class="column has-text-centered" style="text-align: center;"></div>
          <div class="publication-links"
            style="display: flex; justify-content: center; align-items: center; gap: 1rem; margin-top: 1.2rem; flex-wrap: wrap;">
            
            <a href="https://pdf.sciencedirectassets.com/782702/1-s2.0-S2949719123X0003X/1-s2.0-S2949719123000201/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDjiEH5Z0dS%2Fj1i%2BxvE%2Ft5LBmE1ENYhH4ow8kj9e%2BBOaQIgQUSZrR%2B%2Ft8f%2FA6phyddv98lb4hMipL1C8GT8kTp9RxgquwUIk%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDNC91mlZ87jLF70onCqPBbWZwRC18c%2Bwj3hYfc43bQ0AQgKt3mG34VsauFEJniQrZ4T4jUUylOa6WsxfZwc7UO%2B70Fxl6W6Cg5tr2iUdrHXDCPiCrj8yHiPbZKuEbb7bYvY7OW8v9%2FsBfQrkItNPlzoDp6RCqsyvy4Q%2FfQBBYyuNPqiBipK11V76rcFyN1Abm0IKFAaKvWj3XTkTAXMxHPzKvfN9Lr5yLRrG%2BwkBm5HwaWkHTj5XpQq8D2B%2B77IUlcbsoOS5Qh9lCXC%2By0RMsFct1m%2BEBR6QTNsuSA1kC8LenFpUYFbAow6SHg3EkTNlmgHheB4PLmUnsJvTkb%2FQ0eyStGwZuV4aAMlb8pofIdSo4G9%2Fc4aXpdb2RAvfMNp3XQEl%2BDOUSFbdgQBLhwgFXPVxLrQXnVa%2FOnBkajtKw5AQ4%2F8pWAOGa5I76eHyYFYGIMpZXHCs7tT3Nm00HDKqeeORsQCK0YO2YNAtctYs3ydZtxe5LsXGPuOfcHJH6hlhHaPSZy18ppoaLLvjkAL0S7Pafu7rCA3rJ%2FTCnXzBlP6LRc9d%2BdfBhL9elZ5%2FpseQ8VyEdG%2FXzz5Nef6Qfvo1zyfMwon%2FD7MZ13J3IHvawKY3fm9PtEi2XwW2pWKeZTJRgFXO9vX4z2ApRPER88McQuTWtYEh9qnGkmSYiEPyVf3%2B6R57gwGS%2Ft65Tz%2FRRfNh1O4XY1D%2BkoV0tUp6KvLnhdpPy2CRVwqXJFuMvUxpjrJ1wMjSXKnAA4AEGJMUn8BtI5HVh%2FepUpYRThCNcDd4w4V1Reo8ZIFbesi%2FQjg7npuwT%2BBp%2Fe5TcLHbm%2BGnumkGlb6OXMwcgsJKDUXC1UcSLREQik8q8c%2BdU5kcB8UEmWzdA%2BkXZHB%2BmoBua0k246gwprHiwQY6sQHj6OleUpPWYNW58w%2Bbw1GO9wtJM8TisQndk5m79BkBoYHTkk%2FIYeKuct9qMN2fFld2zvZAzQHXgg9whi%2FJUMiId2fuPUCLbjEux4Yas5q4s0j6r9SHWWYQtPQmfrhGCKYFVPcKJHj9b9DEn%2FVRrJlWs2hUpCGu4OZmhVSCOiYo%2BMbCN2JB6XMijERWRAsupxuKc8LNUUrWw4VJwRvH9J1pvnaG6s99m76jhRIsaxPHf%2FE%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250529T174619Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7VDOOTEW%2F20250529%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=eed499eb99fc51ba229e75b5ce220d62173babc73c883628dab6bfbcb44d345b&hash=b02db3f58191bdabed2d0fff57d7660f3b7071a51026fab2446395e7e57e3cb9&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S2949719123000201&tid=spdf-a94ad33c-ca53-4fd4-9dc2-95b4235be9e8&sid=1b380ad850dfe64f006ad0e9fa857c99cb79gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=130f5754575806035507&rr=9477cf7c9f9a9a9b&cc=in" target="_blank" class="button is-rounded is-medium"
              style="background: linear-gradient(90deg, #ff9800 0%, #ff5722 100%); color: #fff; border: none; box-shadow: 0 4px 12px rgba(255,152,0,0.15); display: flex; align-items: center; justify-content: center; gap: 0.6rem; font-weight: 600; min-width: 120px; min-height: 48px; border-radius: 18px;">
              <span class="icon"
                style="display: flex; align-items: center; justify-content: center; background: #fff3e0; border-radius: 50%; width: 2.2em; height: 2.2em;">
                <svg width="1.3em" height="1.3em" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <rect width="24" height="24" rx="6" fill="none" />
                  <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z" stroke="#ff5722"
                    stroke-width="2" />
                  <polyline points="14 2 14 8 20 8" stroke="#ff9800" stroke-width="2" fill="none" />
                  <line x1="16" y1="13" x2="8" y2="13" stroke="#ff9800" stroke-width="2" />
                  <line x1="16" y1="17" x2="8" y2="17" stroke="#ff9800" stroke-width="2" />
                  <polyline points="10 9 9 9 8 9" stroke="#ff9800" stroke-width="2" fill="none" />
                </svg>
              </span>
              <span style="display: flex; align-items: center; justify-content: center;">Paper</span>
            </a>
            
            
            
            <a href="https://www.sciencedirect.com/science/article/pii/S2949719123000201" target="_blank" class="button is-rounded is-medium"
              style="background: linear-gradient(90deg, #43cea2 0%, #185a9d 100%); color: #fff; border: none; box-shadow: 0 4px 12px rgba(67,206,162,0.15); display: flex; align-items: center; justify-content: center; gap: 0.6rem; font-weight: 600; min-width: 120px; min-height: 48px; border-radius: 18px;">
              <span class="icon"
                style="display: flex; align-items: center; justify-content: center; background: #e0f7fa; border-radius: 50%; width: 2.2em; height: 2.2em;">
                <svg width="1.3em" height="1.3em" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <circle cx="12" cy="12" r="10" stroke="#185a9d" stroke-width="2" fill="#fff" />
                  <path d="M8 12a4 4 0 0 1 8 0c0 2.21-1.79 4-4 4s-4-1.79-4-4z" stroke="#43cea2" stroke-width="2"
                    fill="none" />
                  <path d="M12 2v2m0 16v2m10-10h-2M4 12H2" stroke="#43cea2" stroke-width="2" stroke-linecap="round" />
                </svg>
              </span>
              <span style="display: flex; align-items: center; justify-content: center;">URL</span>
            </a>
            
            
            <a href="" target="_blank" class="button is-rounded is-medium"
              style="background: linear-gradient(90deg, #ff512f 0%, #dd2476 100%); color: #fff; border: none; box-shadow: 0 4px 12px rgba(221,36,118,0.15); display: flex; align-items: center; justify-content: center; gap: 0.6rem; font-weight: 600; min-width: 120px; min-height: 48px; border-radius: 18px;">
              <span class="icon"
                style="display: flex; align-items: center; justify-content: center; background: #ffe0ef; border-radius: 50%; width: 2.2em; height: 2.2em;">
                <svg width="1.3em" height="1.3em" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <circle cx="12" cy="12" r="10" stroke="#dd2476" stroke-width="2" fill="#fff" />
                  <polygon points="10,8 16,12 10,16" fill="#ff512f" />
                </svg>
              </span>
              <span style="display: flex; align-items: center; justify-content: center;">Video</span>
            </a>
            
            
            
            
            
          </div>
        </div>
      </div>
    </div>
    </div>
    </div>
  </section>
  
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Natural Language Inference (NLI) has been one of the fundamental tasks in Natural Language Processing (NLP). Recognizing Textual Entailment (RTE) between the two pieces of text is a crucial problem. It adds further challenges when it involves two languages, i.e., in the cross-lingual scenario. In this paper, we propose VisioTextual-Attention (VTA) — an effective visual–textual coattention mechanism for multi-modal crosslingual NLI. Through our research, we show that instead of using only linguistic input features, introducing visual features supporting the textual inputs improves the performance of NLI models if an effective cross-modal attention mechanism is carefully constructed. We perform several experiments on a standard cross-lingual textual entailment dataset in Hindi–English language pairs and show that the addition of visual information to the dataset along with our proposed VisioTextual Attention (VTA) enhances performance and surpasses the current state-of-the-art by 4.5%. Through monolingual experiments, we also show that the proposed VTA mechanism surpasses monolingual state-of-the-art by a margin of 2.89%. We argue that our VTA mechanism is model agnostic and can be used with other deep learning-based architectures for grounded cross-lingual NLI.</p>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  
  
  <div class="spotlight-image-wrapper" style="width: 100%; text-align: center; margin: 1.5rem 0">
    <img src="/nlip/publications/images/vta-ex.png" alt="Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention image"
      style="max-width: 100%; max-height: 400px; display: inline-block; border-radius: 8px;" />
  </div>
  
  
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content" style="position: relative; padding: 1rem;">
      <h2 class="title">BibTeX</h2>
      <div style="position: relative;"><div style="overflow-x: auto; width: 100%;">
          <pre id="bibtex-block"
            style="background: #f7f7f9; border: 1px solid #e1e1e8; border-radius: 8px; padding: 1em 1.5em; font-family: 'JetBrains Mono', 'Fira Mono', 'Menlo', 'Consolas', 'Liberation Mono', 'monospace'; font-size: 1.08em; line-height: 1.4; color: #2d2d2d; margin: 0; min-height: 0; height: auto; display: block; text-align: left; white-space: pre; tab-size: 2;">@article{DE2023100023,
    title    = {Towards Improvement of Grounded Cross-lingual Natural Language Inference with VisioTextual Attention},
    journal  = {Natural Language Processing Journal},
    volume   = {4},
    pages    = {100023},
    year     = {2023},
    issn     = {2949-7191},
    doi      = {https://doi.org/10.1016/j.nlp.2023.100023},
    url      = {https://www.sciencedirect.com/science/article/pii/S2949719123000201},
    author   = {Arkadipta De and Maunendra Sankar Desarkar and Asif Ekbal},
    keywords = {Attention mechanism, Textual entailment, Natural Language Inference, Grounded textual entailment, Cross-lingual textual entailment},
    abstract = {Natural Language Inference (NLI) has been one of the fundamental tasks in Natural Language Processing (NLP). Recognizing Textual Entailment (RTE) between the two pieces of text is a crucial problem. It adds further challenges when it involves two languages, i.e., in the cross-lingual scenario. In this paper, we propose VisioTextual-Attention (VTA) — an effective visual–textual coattention mechanism for multi-modal cross-lingual NLI. Through our research, we show that instead of using only linguistic input features, introducing visual features supporting the textual inputs improves the performance of NLI models if an effective cross-modal attention mechanism is carefully constructed. We perform several experiments on a standard cross-lingual textual entailment dataset in Hindi–English language pairs and show that the addition of visual information to the dataset along with our proposed VisioTextual Attention (VTA) enhances performance and surpasses the current state-of-the-art by 4.5%. Through monolingual experiments, we also show that the proposed VTA mechanism surpasses monolingual state-of-the-art by a margin of 2.89%. We argue that our VTA mechanism is model agnostic and can be used with other deep learning-based architectures for grounded cross-lingual NLI.}
}</pre>
        </div>
      </div>
    </div>
  </section>
  

  <!-- Scripts from default.html's body (to be placed before closing </body>) -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.2/dist/umd/popper.min.js"
    integrity="sha384-IQsoLXl5PILFhosVNubq5LC7Qb9DXgDA9i+tQ8Zj3iwWAwPtgFTxbJ8NT4GN1R8p"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.min.js"
    integrity="sha384-cVKIPhGWiC2Al4u+LWgxfKTRIcfu0JTxR+EQDz/bgldoEyl4H0zUF0QKbrJ0EcQF"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.11.3/viewer.min.js"
    integrity="sha512-f8kZwYACKF8unHuRV7j/5ILZfflRncxHp1f6y/PKuuRpCVgpORNZMne1jrghNzTVlXabUXIg1iJ5PvhuAaau6Q=="
    crossorigin="anonymous" referrerpolicy="no-referrer"></script>
</body>

</html>