<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-Context Information for Word Representation Learning - IIT Hyderabad</title>

  <!-- CSS from default.html -->
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,600">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
  <link href="/nlip/css/zoom.css" rel="stylesheet">
  <link rel="stylesheet" href="/nlip/style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.11.3/viewer.min.css"
    integrity="sha512-zdX1vpRJcExqoI7yuYbSFAbSWxscAoLF0KoUPvMSAK09BaOZ47UFdP4ABSXpevKfcD0MTVxvh0jLHQ=="
    crossorigin="anonymous" referrerpolicy="no-referrer" />

  <!-- Favicons: Prioritize spotlight's, then default's -->
  <link rel="icon" type="image/x-icon" href="/nlip/static/images/ns.png">
  <!-- From original spotlight.html -->
  <link rel="shortcut icon" href="/nlip/favicon.png"> <!-- From default.html -->

  <!-- OG Meta Tags: Combine and prioritize page-specific from spotlight -->
  <meta name="description" content="Multi-Context Information for Word Representation Learning"> <!-- From original spotlight.html -->
  <meta property="og:title" content="Multi-Context Information for Word Representation Learning" /> <!-- From original spotlight.html -->
  <meta property="og:description" content="Word embedding techniques in literature are mostly based on Bag of Words models where words that co-occur with each other are considered to be related. Howev..." />
  <!-- From original spotlight.html -->
  <meta property="og:url" content="nlip-lab.github.io/nlip/publications/multi-context-info/" />
  <!-- From original spotlight.html -->
  <meta property="og:image" content="/nlip/publications/images/multi-context-info.png" />
  <!-- From original spotlight.html -->
  <meta property="og:image:width" content="1200" /> <!-- From original spotlight.html -->
  <meta property="og:image:height" content="630" /> <!-- From original spotlight.html -->
  <meta property="og:locale" content="en_US"> <!-- From default.html -->
  <meta property="og:type" content="article"> <!-- From default.html -->
  <meta property="og:site_name" content="NLIP: Natural Language and Information Processing Lab"> <!-- From default.html -->

  <!-- CSS from original spotlight.html -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="/nlip/static/css/bulma.min.css">
  <link rel="stylesheet" href="/nlip/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="/nlip/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="/nlip/static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/nlip/static/css/index.css">

  <!-- Scripts for head from default.html -->
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script src="//code.jquery.com/jquery-1.12.0.min.js"></script>
  <script src="/nlip/js/zoom.js"></script>
  <script src="/nlip/js/transition.js"></script>

  <style>
    /* Styles from default.html (general site styles) */
    .carousel {
      border-radius: 10px;
      overflow: hidden;
    }

    .grid-container {
      display: grid;
      gap: 5px;
      grid-template-columns: auto auto auto auto;
      padding: 10px;
    }

    .grid-item {
      padding: 10px;
      text-align: center;
    }

    /* Styles from original spotlight.html (specific to spotlight pages) */
    .image-card {
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
      overflow: hidden;
      transition: transform 0.2s ease-in-out;
    }

    .image-card:hover {
      transform: scale(1.05);
    }

    .image-table {
      margin: 30px auto;
      text-align: center;
    }
  </style>
</head>

<body id="page-top">
  <section class="hero">
    <!-- <div class="hero-body" style="padding-top: 2.5rem;"> Add space above title -->
    <div class="container is-max-desktop" style="padding: 2rem;">
      <div class="columns is-centered">
        <div class="column has-text-centered" style="text-align: center;">
          <!-- Center align everything before abstract -->
          <h1 class="title is-1 publication-title" style="text-align: center;">Multi-Context Information for Word Representation Learning</h1>
          <div class="is-size-5 publication-authors" style="text-align: center;">
            <p>Swapnil Dewalkar and Maunendra Sankar Desarkar</p>

          </div>
          <div class="is-size-5 publication-authors" style="text-align: center;">
            

            <br><span style="font-size:1.3rem; font-weight:700; color:#2c3e50;">DocEng 2019: Proceedings of the ACM Symposium on Document Engineering 2019</span>
          </div>
          <div class="column has-text-centered" style="text-align: center;"></div>
          <div class="publication-links"
            style="display: flex; justify-content: center; align-items: center; gap: 1rem; margin-top: 1.2rem; flex-wrap: wrap;">
            
            <a href="https://dl.acm.org/doi/pdf/10.1145/3342558.3345418" target="_blank" class="button is-rounded is-medium"
              style="background: linear-gradient(90deg, #ff9800 0%, #ff5722 100%); color: #fff; border: none; box-shadow: 0 4px 12px rgba(255,152,0,0.15); display: flex; align-items: center; justify-content: center; gap: 0.6rem; font-weight: 600; min-width: 120px; min-height: 48px; border-radius: 18px;">
              <span class="icon"
                style="display: flex; align-items: center; justify-content: center; background: #fff3e0; border-radius: 50%; width: 2.2em; height: 2.2em;">
                <svg width="1.3em" height="1.3em" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <rect width="24" height="24" rx="6" fill="none" />
                  <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z" stroke="#ff5722"
                    stroke-width="2" />
                  <polyline points="14 2 14 8 20 8" stroke="#ff9800" stroke-width="2" fill="none" />
                  <line x1="16" y1="13" x2="8" y2="13" stroke="#ff9800" stroke-width="2" />
                  <line x1="16" y1="17" x2="8" y2="17" stroke="#ff9800" stroke-width="2" />
                  <polyline points="10 9 9 9 8 9" stroke="#ff9800" stroke-width="2" fill="none" />
                </svg>
              </span>
              <span style="display: flex; align-items: center; justify-content: center;">Paper</span>
            </a>
            
            
            
            <a href="https://dl.acm.org/doi/10.1145/3342558.3345418" target="_blank" class="button is-rounded is-medium"
              style="background: linear-gradient(90deg, #43cea2 0%, #185a9d 100%); color: #fff; border: none; box-shadow: 0 4px 12px rgba(67,206,162,0.15); display: flex; align-items: center; justify-content: center; gap: 0.6rem; font-weight: 600; min-width: 120px; min-height: 48px; border-radius: 18px;">
              <span class="icon"
                style="display: flex; align-items: center; justify-content: center; background: #e0f7fa; border-radius: 50%; width: 2.2em; height: 2.2em;">
                <svg width="1.3em" height="1.3em" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <circle cx="12" cy="12" r="10" stroke="#185a9d" stroke-width="2" fill="#fff" />
                  <path d="M8 12a4 4 0 0 1 8 0c0 2.21-1.79 4-4 4s-4-1.79-4-4z" stroke="#43cea2" stroke-width="2"
                    fill="none" />
                  <path d="M12 2v2m0 16v2m10-10h-2M4 12H2" stroke="#43cea2" stroke-width="2" stroke-linecap="round" />
                </svg>
              </span>
              <span style="display: flex; align-items: center; justify-content: center;">URL</span>
            </a>
            
            
            
            
            
            
          </div>
        </div>
      </div>
    </div>
    </div>
    </div>
  </section>
  
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>Word embedding techniques in literature are mostly based on Bag of Words models where words that co-occur with each other are considered to be related. However, it is not necessary for similar or related words to occur in the same context window. In this paper, we propose a new approach to combine different types of resources for training word embeddings. The lexical resources used in this work are Dependency Parse Tree and WordNet. Apart from the co-occurrence information, the use of these additional resources helps us in including the semantic and syntactic information from the text in learning the word representations. The learned representations are evaluated on multiple evaluation tasks like Semantic Textual Similarity, Word Similarity. Results of the experimental analyses highlight the usefulness of the proposed methodology.</p>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  
  
  <div class="spotlight-image-wrapper" style="width: 100%; text-align: center; margin: 1.5rem 0">
    <img src="/nlip/publications/images/multi-context-info.png" alt="Multi-Context Information for Word Representation Learning image"
      style="max-width: 100%; max-height: 400px; display: inline-block; border-radius: 8px;" />
  </div>
  
  
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content" style="position: relative; padding: 1rem;">
      <h2 class="title">BibTeX</h2>
      <div style="position: relative;"><div style="overflow-x: auto; width: 100%;">
          <pre id="bibtex-block"
            style="background: #f7f7f9; border: 1px solid #e1e1e8; border-radius: 8px; padding: 1em 1.5em; font-family: 'JetBrains Mono', 'Fira Mono', 'Menlo', 'Consolas', 'Liberation Mono', 'monospace'; font-size: 1.08em; line-height: 1.4; color: #2d2d2d; margin: 0; min-height: 0; height: auto; display: block; text-align: left; white-space: pre; tab-size: 2;">@inproceedings{10.1145/3342558.3345418,
    author    = {Dewalkar, Swapnil and Desarkar, Maunendra Sankar},
    title     = {Multi-Context Information for Word Representation Learning},
    year      = {2019},
    isbn      = {9781450368872},
    publisher = {Association for Computing Machinery},
    address   = {New York, NY, USA},
    url       = {https://doi.org/10.1145/3342558.3345418},
    doi       = {10.1145/3342558.3345418},
    abstract  = {Word embedding techniques in literature are mostly based on Bag of Words models where words that co-occur with each other are considered to be related. However, it is not necessary for similar or related words to occur in the same context window. In this paper, we propose a new approach to combine different types of resources for training word embeddings. The lexical resources used in this work are Dependency Parse Tree and WordNet. Apart from the co-occurrence information, the use of these additional resources helps us in including the semantic and syntactic information from the text in learning the word representations. The learned representations are evaluated on multiple evaluation tasks like Semantic Textual Similarity, Word Similarity. Results of the experimental analyses highlight the usefulness of the proposed methodology.},
    booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
    articleno = {21},
    numpages  = {4},
    keywords  = {Dependency Parsing, Representation Learning, Word embedding, WordNet},
    location  = {Berlin, Germany},
    series    = {DocEng &#39;19}
}</pre>
        </div>
      </div>
    </div>
  </section>
  

  <!-- Scripts from default.html's body (to be placed before closing </body>) -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.2/dist/umd/popper.min.js"
    integrity="sha384-IQsoLXl5PILFhosVNubq5LC7Qb9DXgDA9i+tQ8Zj3iwWAwPtgFTxbJ8NT4GN1R8p"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.min.js"
    integrity="sha384-cVKIPhGWiC2Al4u+LWgxfKTRIcfu0JTxR+EQDz/bgldoEyl4H0zUF0QKbrJ0EcQF"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.11.3/viewer.min.js"
    integrity="sha512-f8kZwYACKF8unHuRV7j/5ILZfflRncxHp1f6y/PKuuRpCVgpORNZMne1jrghNzTVlXabUXIg1iJ5PvhuAaau6Q=="
    crossorigin="anonymous" referrerpolicy="no-referrer"></script>
</body>

</html>