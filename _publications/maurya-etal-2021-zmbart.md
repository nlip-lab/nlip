---
layout: publication_spotlight
collection: publications
key: maurya-etal-2021-zmbart
title: "ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation"
authors: "Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Yoshinobu Kano, and Kumari Deepshikha"
journal: "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
year: 2021
month: 8
highlight: 1
link: "https://aclanthology.org/2021.findings-acl.248/"
pdf: "https://aclanthology.org/2021.findings-acl.248.pdf"
code: "https://github.com/kaushal0494/ZmBART"
img: "maurya-etal-2021-zmbart.png"
video: "https://aclanthology.org/2021.findings-acl.248.mp4"
bibtex: 1
summary: "We propose an unsupervised crosslingual language generation framework (called ZmBART) that does not use any parallel or pseudo-parallel/back-translated data. In this framework, we further pre-train mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task using monolingual data of three languages. The objective function of the auxiliary task is close to the target tasks which enriches the multi-lingual latent representation of mBART and provides good initialization for target tasks. Then, this model is fine-tuned with task-specific supervised English data and directly evaluated with low-resource languages in the Zero-shot setting."
---
