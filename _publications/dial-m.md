---
layout: publication_spotlight
collection: publications
key: dial-m
title: "Dial-M: A Masking-based Framework for Dialogue Evaluation"
authors: "Suvodip Dey and Maunendra Sankar Desarkar"
journal: "24th Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2023)<br><br><i> Nominated for Best Paper Award</i>"
abstract: "In dialogue systems, automatically evaluating machine-generated responses is critical and challenging. Despite the tremendous progress in dialogue generation research, its evaluation heavily depends on human judgments. The standard word-overlapping based evaluation metrics are ineffective for dialogues. As a result, most of the recently proposed metrics are model-based and reference-free, which learn to score different aspects of a conversation. However, understanding each aspect requires a separate model, which makes them computationally expensive. To this end, we propose Dial-M, a Masking-based reference-free framework for Dialogue evaluation. The main idea is to mask the keywords of the current utterance and predict them, given the dialogue history and various conditions (like knowledge, persona, etc.), thereby making the evaluation framework simple and easily extensible for multiple datasets. Regardless of its simplicity, Dial-M achieves comparable performance to state-of-the-art metrics on several dialogue evaluation datasets. We also discuss the interpretability of our proposed metric along with error analysis."
year: 2023
month: 7
highlight: 1
url: "https://aclanthology.org/2023.sigdial-1.7"
pdf: "https://aclanthology.org/2023.sigdial-1.7.pdf"
code: "https://github.com/SuvodipDey/Dial-M"
img: "dial-m_pic.png"
bibtex: 1
summary: "We propose Dial-M, a Masking-based reference-free framework for Dialogue evaluation. The main idea is to mask the keywords of the current utterance and predict them, given the dialogue history and various conditions (like knowledge, persona, etc.), thereby making the evaluation framework simple and easily extensible for multiple datasets. Regardless of its simplicity, Dial-M achieves comparable performance to state-of-the-art metrics on several dialogue evaluation datasets. We also discuss the interpretability of our proposed metric along with error analysis."
---
