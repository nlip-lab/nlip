# Year 2025

- key: "inherent-ability"
  authors: "Aishwarya Maheswaran, Caslon Chua, Maunendra Sankar Desarkar"
  title: "Probing the Inherent Ability of Large Language Models for Generating Empathetic Responses"
  journal: "2025 IEEE Swiss Conference on Data Science (SDS)"
  abstract: 'Large Language Models (LLMs) have demonstrated capabilities beyond basic text generation, like question answering, translation, and even stylistic text generation. Since these models are available for public use, enormous effort has been put into safety engineering to ensure that undesirable and harmful text is not generated and the generations are polite and empathetic. In this work, we examine the inherent empathy capabilities of five open-source LLMs and evaluate them from multiple angles using automated metrics to understand their capabilities and limitations. In the context of this work, “inherent” refers to the LLM’s ability to generate empathetic text without having to explicitly prompt for it. We examine if empathy is treated as a style change or is the model demonstrating some understanding of the specific user’s context. We find that LLMs use more emotion words than humans in their generations. They can also infer the user’s emotional state, a crucial characteristic of empathy. Due to the probabilistic nature of obtaining generations, there is a tendency for the responses to drift away from the user’s actual intent. In such cases, specific prompting allows the model to respond appropriately. We summarize the differences observed between human and LLM generations and conclude with a potential research direction for empathetic dialog generation that leverages the capabilities of LLMs.'
  year: 2025
  month: 6
  highlight: 1
  img: "inherent-ability.png"
  bibtex: inherent-ability
  url: "https://ieeexplore.ieee.org/document/11081499"
  pdf: "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11081499"
  summary: 'Large Language Models (LLMs) have demonstrated capabilities beyond basic text generation, like question answering, translation, and even stylistic text generation. Since these models are available for public use, enormous effort has been put into safety engineering to ensure that undesirable and harmful text is not generated and the generations are polite and empathetic. In this work, we examine the inherent empathy capabilities of five open-source LLMs and evaluate them from multiple angles using automated metrics to understand their capabilities and limitations. In the context of this work, “inherent” refers to the LLM’s ability to generate empathetic text without having to explicitly prompt for it. We examine if empathy is treated as a style change or is the model demonstrating some understanding of the specific user’s context. We find that LLMs use more emotion words than humans in their generations. They can also infer the user’s emotional state, a crucial characteristic of empathy. Due to the probabilistic nature of obtaining generations, there is a tendency for the responses to drift away from the user’s actual intent. In such cases, specific prompting allows the model to respond appropriately. We summarize the differences observed between human and LLM generations and conclude with a potential research direction for empathetic dialog generation that leverages the capabilities of LLMs.'

- key: "morphtok"
  authors: "Maharaj Brahma, N J Karthika, Atul Kumar Singh, Devaraja Adiga, Smruti Bhate, Ganesh Ramakrishnan, Rohit Saluja, Maunendra Sankar Desarkar"
  title: "MorphTok: Morphologically Grounded Tokenization for Indic languages"
  journal: "ICML 2025 Tokenization Workshop (TokShop)"
  abstract: 'Tokenization is a crucial step in NLP, especially with the rise of large language models (LLMs), impacting downstream performance, computational cost, and efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE) algorithm for subword tokenization that greedily merges frequent character bigrams, often leading to segmentation that does not align with linguistically meaningful units. To address this, we propose morphology-aware segmentation as a pre-tokenization step before applying BPE. To facilitate morphology-aware segmentation, we create a novel dataset for Hindi and Marathi, incorporating sandhi splitting to enhance the subword tokenization. Experiments on downstream tasks show that morphologically grounded tokenization improves machine translation and language modeling performance. Additionally, to handle the dependent vowels common in syllable-based writing systems used by Indic languages, we propose Constrained BPE (CBPE), an extension to the standard BPE algorithm incorporating script-specific constraints. In particular, CBPE handles dependent vowels to form a cohesive unit with other characters instead of occurring as a single unit. Our results show that CBPE achieves a 1.68% reduction in fertility scores while maintaining comparable or improved downstream performance in machine translation and language modeling, offering a computationally efficient alternative to standard BPE. Moreover, to evaluate segmentation across different tokenization algorithms, we introduce a new human evaluation metric, \textit{EvalTok}, enabling more human-grounded assessment.'
  year: 2025
  month: 6
  highlight: 1
  img: "morphtok.png"
  bibtex: morphtok
  url: "https://openreview.net/forum?id=32wHtvZOqH"
  pdf: "https://openreview.net/pdf?id=32wHtvZOqH"
  summary: 'Tokenization is a crucial step in NLP, especially with the rise of large language models (LLMs), impacting downstream performance, computational cost, and efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE) algorithm for subword tokenization that greedily merges frequent character bigrams, often leading to segmentation that does not align with linguistically meaningful units. To address this, we propose morphology-aware segmentation as a pre-tokenization step before applying BPE. To facilitate morphology-aware segmentation, we create a novel dataset for Hindi and Marathi, incorporating sandhi splitting to enhance the subword tokenization. Experiments on downstream tasks show that morphologically grounded tokenization improves machine translation and language modeling performance. Additionally, to handle the dependent vowels common in syllable-based writing systems used by Indic languages, we propose Constrained BPE (CBPE), an extension to the standard BPE algorithm incorporating script-specific constraints. In particular, CBPE handles dependent vowels to form a cohesive unit with other characters instead of occurring as a single unit. Our results show that CBPE achieves a 1.68% reduction in fertility scores while maintaining comparable or improved downstream performance in machine translation and language modeling, offering a computationally efficient alternative to standard BPE. Moreover, to evaluate segmentation across different tokenization algorithms, we introduce a new human evaluation metric, \textit{EvalTok}, enabling more human-grounded assessment.'

- key: "notam"
  authors: "Minal Nitin Dani, Aishwarya Maheswaran, Maunendra Sankar Desarkar"
  title: "Semantics-aware prompting for translating NOtices To AirMen"
  journal: "Findings of the Association for Computational Linguistics: ACL 2025"
  abstract: 'A NOTAM or NOtice To AirMen is a crucial notice for different aviation stakeholders, particularly flight crews. It delivers essential notifications about abnormal conditions of Aviation System components such as changes to facilities, hazards, service, procedure that are not known far enough in advance to be publicized through other means. NOTAM messages are short, contain acronyms, and look cryptic in most of the cases. Writing and understanding these messages put heavy cognitive load on its end users. In this work, we take up the task of translating NOTAMs into English natural language using LLMs. Since NOTAMs do not adhere to English grammar rules and have their own decoding rules, large language models (LLMs) cannot translate them without effective prompting. In this paper, we develop a framework to come up with effective prompts to achieve the translations. Our approach uses context-aware semantic prompting techniques, paired with domain-specific rules, to improve the accuracy and clarity of translations. The framework is evaluated using comprehensive experiments (6 LLMs of varying sizes, and with 5 different prompting setups for each) and eight evaluation metrics measuring different aspects of the translation. The results demonstrate that our methodology can produce clear translations that accurately convey the information contained in NOTAMs.' 
  year: 2025
  month: 5
  highlight: 1
  img: "notam.jpg"
  bibtex: notam
  url: "https://aclanthology.org/2025.findings-acl.1253/"
  pdf: "https://aclanthology.org/2025.findings-acl.1253.pdf"
  summary: 'A NOTAM or NOtice To AirMen is a crucial notice for different aviation stakeholders, particularly flight crews. It delivers essential notifications about abnormal conditions of Aviation System components such as changes to facilities, hazards, service, procedure that are not known far enough in advance to be publicized through other means. NOTAM messages are short, contain acronyms, and look cryptic in most of the cases. Writing and understanding these messages put heavy cognitive load on its end users. In this work, we take up the task of translating NOTAMs into English natural language using LLMs. Since NOTAMs do not adhere to English grammar rules and have their own decoding rules, large language models (LLMs) cannot translate them without effective prompting. In this paper, we develop a framework to come up with effective prompts to achieve the translations. Our approach uses context-aware semantic prompting techniques, paired with domain-specific rules, to improve the accuracy and clarity of translations. The framework is evaluated using comprehensive experiments (6 LLMs of varying sizes, and with 5 different prompting setups for each) and eight evaluation metrics measuring different aspects of the translation. The results demonstrate that our methodology can produce clear translations that accurately convey the information contained in NOTAMs.'


# Year 2024

- key: "bok"
  authors: "Suvodip Dey, Maunendra Sankar Desarkar"
  title: "BoK: Introducing Bag-of-Keywords Loss for Interpretable Dialogue Response Generation"
  journal: "25th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2024)"
  abstract: "The standard language modeling (LM) loss by itself has been shown to be inadequate for effective dialogue modeling. As a result, various training approaches, such as auxiliary loss functions and leveraging human feedback, are being adopted to enrich open-domain dialogue systems. One such auxiliary loss function is Bag-of-Words (BoW) loss, defined as the cross-entropy loss for predicting all the words/tokens of the next utterance. In this work, we propose a novel auxiliary loss named Bag-of-Keywords (BoK) loss to capture the central thought of the response through keyword prediction and leverage it to enhance the generation of meaningful and interpretable responses in open-domain dialogue systems. BoK loss upgrades the BoW loss by predicting only the keywords or critical words/tokens of the next utterance, intending to estimate the core idea rather than the entire response. We incorporate BoK loss in both encoder-decoder (T5) and decoder-only (DialoGPT) architecture and train the models to minimize the weighted sum of BoK and LM (BoK-LM) loss. We perform our experiments on two popular open-domain dialogue datasets, DailyDialog and Persona-Chat. We show that the inclusion of BoK loss improves the dialogue generation of backbone models while also enabling post-hoc interpretability. We also study the effectiveness of BoK-LM loss as a reference-free metric and observe comparable performance to the state-of-the-art metrics on various dialogue evaluation datasets."
  year: 2024
  month: 9
  highlight: 1
  img: "bok.png"
  bibtex: bok
  url: "https://aclanthology.org/2024.sigdial-1.48/"
  pdf: "https://aclanthology.org/2024.sigdial-1.48.pdf"
  code: "https://github.com/SuvodipDey/BoK"
  summary: "The standard language modeling (LM) loss by itself has been shown to be inadequate for effective dialogue modeling. As a result, various training approaches, such as auxiliary loss functions and leveraging human feedback, are being adopted to enrich open-domain dialogue systems. One such auxiliary loss function is Bag-of-Words (BoW) loss, defined as the cross-entropy loss for predicting all the words/tokens of the next utterance. In this work, we propose a novel auxiliary loss named Bag-of-Keywords (BoK) loss to capture the central thought of the response through keyword prediction and leverage it to enhance the generation of meaningful and interpretable responses in open-domain dialogue systems. BoK loss upgrades the BoW loss by predicting only the keywords or critical words/tokens of the next utterance, intending to estimate the core idea rather than the entire response. We incorporate BoK loss in both encoder-decoder (T5) and decoder-only (DialoGPT) architecture and train the models to minimize the weighted sum of BoK and LM (BoK-LM) loss. We perform our experiments on two popular open-domain dialogue datasets, DailyDialog and Persona-Chat. We show that the inclusion of BoK loss improves the dialogue generation of backbone models while also enabling post-hoc interpretability. We also study the effectiveness of BoK-LM loss as a reference-free metric and observe comparable performance to the state-of-the-art metrics on various dialogue evaluation datasets."

- key: "dac"
  authors: "Aishwarya Maheswaran, Kaushal Kumar Maurya, Manish Gupta, Maunendra Sankar Desarkar"
  title: "DAC: Quantized Optimal Transport Reward-based Reinforcement Learning Approach to Detoxify Query Auto-Completion"
  journal: "47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024)"
  abstract: "Modern Query Auto-Completion (QAC) systems utilize natural language generation (NLG) using large language models (LLM) to achieve remarkable performance. However, these systems are prone to generating biased and toxic completions due to inherent learning biases. Existing detoxification approaches exhibit two key limitations: (1) They primarily focus on mitigating toxicity for grammatically well-formed long sentences but struggle to adapt to the QAC task, where queries are short and structurally different (include spelling errors, do not follow grammatical rules and have relatively flexible word order). (2) These approaches often view detoxification through a binary lens where all text labeled as toxic is undesirable and non-toxic is considered desirable. To address these limitations, we propose DAC, an intuitive and efficient reinforcement learning-based model to detoxify QAC. With DAC, we introduce an additional perspective of considering the third query class of addressable toxicity. These queries can encompass implicit toxicity, subjective toxicity, or non-toxic queries containing toxic words. We incorporate this three-class query behavior perspective into the proposed model through quantized optimal transport to learn distinctions and generate truly non-toxic completions. We evaluate toxicity levels in the generated completions by DAC across two real-world QAC datasets (Bing and AOL) using two classifiers: a publicly available generic classifier (Detoxify) and a search query-specific classifier, which we develop (TClassify). we find that DAC consistently outperforms all existing baselines on the Bing dataset and achieves competitive performance on the AOL dataset for query detoxification. % providing high quality and low toxicity. We make the code and models publicly available."
  year: 2024
  month: 7
  highlight: 1
  img: "DAC-SIGIR-2024.png"
  url: "https://dl.acm.org/doi/10.1145/3626772.3657779"
  pdf: "https://dl.acm.org/doi/pdf/10.1145/3626772.3657779"
  bibtex: dac
  summary: "Modern Query Auto-Completion (QAC) systems utilize natural language generation (NLG) using large language models (LLM) to achieve remarkable performance. However, these systems are prone to generating biased and toxic completions due to inherent learning biases. Existing detoxification approaches exhibit two key limitations: (1) They primarily focus on mitigating toxicity for grammatically well-formed long sentences but struggle to adapt to the QAC task, where queries are short and structurally different (include spelling errors, do not follow grammatical rules and have relatively flexible word order). (2) These approaches often view detoxification through a binary lens where all text labeled as toxic is undesirable and non-toxic is considered desirable. To address these limitations, we propose DAC, an intuitive and efficient reinforcement learning-based model to detoxify QAC. With DAC, we introduce an additional perspective of considering the third query class of addressable toxicity. These queries can encompass implicit toxicity, subjective toxicity, or non-toxic queries containing toxic words. We incorporate this three-class query behavior perspective into the proposed model through quantized optimal transport to learn distinctions and generate truly non-toxic completions. We evaluate toxicity levels in the generated completions by DAC across two real-world QAC datasets (Bing and AOL) using two classifiers: a publicly available generic classifier (Detoxify) and a search query-specific classifier, which we develop (TClassify). we find that DAC consistently outperforms all existing baselines on the Bing dataset and achieves competitive performance on the AOL dataset for query detoxification. % providing high quality and low toxicity. We make the code and models publicly available."

- key: "dqac"
  authors: "Aishwarya Maheswaran, Kaushal Kumar Maurya, Manish Gupta, Maunendra Sankar Desarkar."
  title: "DQAC: Detoxifying Query Auto-Completion with Adapters"
  journal: "28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2024)"
  abstract: "Recent Query Auto-completion (QAC) systems  leverage natural language generation or pre-trained language models (PLMs) to demonstrate remarkable performance. However, these systems also suffer from biased and toxic completions. Efforts have been made to address language detoxification within PLMs using controllable text generation (CTG) techniques, involving training with non-toxic data and employing decoding time approaches. As the completions for QAC systems are usually short, these existing CTG methods based on decoding and training are not directly transferable. Towards these concerns, we propose the first public QAC detoxification model, Detoxifying Query Auto-Completion (or DQAC), which utilizes adapters in a CTG framework. DQAC operates on latent representations with no additional overhead. It leverages two adapters for toxic and non-toxic cases. During inference, we fuse these representations in a controlled manner that guides the generation of query completions towards non-toxicity. We evaluate toxicity levels in the generated completions across two real-world datasets using two classifiers: a publicly available (Detoxify) and a search query-specific classifier which we develop (QDetoxify). DQAC consistently outperforms all existing baselines and emerges as a state-of-the-art model providing high quality and low toxicity. We make the code publicly available at https://shorturl.at/zJ024"
  year: 2024
  month: 5
  highlight: 0
  img: "DQAC-PAKDD-2024.png"
  url: "https://link.springer.com/chapter/10.1007/978-981-97-2266-2_9"
  bibtex: dqac
  summary: "Recent Query Auto-completion (QAC) systems  leverage natural language generation or pre-trained language models (PLMs) to demonstrate remarkable performance. However, these systems also suffer from biased and toxic completions. Efforts have been made to address language detoxification within PLMs using controllable text generation (CTG) techniques, involving training with non-toxic data and employing decoding time approaches. As the completions for QAC systems are usually short, these existing CTG methods based on decoding and training are not directly transferable. Towards these concerns, we propose the first public QAC detoxification model, Detoxifying Query Auto-Completion (or DQAC), which utilizes adapters in a CTG framework. DQAC operates on latent representations with no additional overhead. It leverages two adapters for toxic and non-toxic cases. During inference, we fuse these representations in a controlled manner that guides the generation of query completions towards non-toxicity. We evaluate toxicity levels in the generated completions across two real-world datasets using two classifiers: a publicly available (Detoxify) and a search query-specific classifier which we develop (QDetoxify). DQAC consistently outperforms all existing baselines and emerges as a state-of-the-art model providing high quality and low toxicity. We make the code publicly available at https://shorturl.at/zJ024"

- key: "ticod"
  authors: "Debolena Basak, P. K. Srijith, and Maunendra Sankar Desarkar"
  title: "Transformer Based Multitask Learning for Image Captioning and Object Detection"
  journal: "28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2024)"
  abstract: "In several real-world scenarios like autonomous navigation and mobility, to obtain a better visual understanding of the surroundings, image captioning and object detection play a crucial role. This work introduces a novel multitask learning framework that combines image captioning and object detection intoa joint model. We propose TICOD, Transformer-based Image Captioning andObject Detection model for jointly training both tasks by combining the losses obtained from image captioning and object detection networks. By leveraging joint training, the model benefits from the complementary information shared AQ1between the two tasks, leading to improved performance for image captioning.Our approach utilizes a transformer-based architecture that enables end-to-end AQ2network integration for image captioning and object detection and performs bothtasks jointly. We evaluate the effectiveness of our approach through comprehensive experiments on the MS-COCO dataset. Our model outperforms the baselinesfrom image captioning literature by achieving a 3.65% improvement in BERTScore."
  year: 2024
  month: 5
  highlight: 0
  img: "TICOD-PAKDD-2024.png"
  url: "https://dl.acm.org/doi/10.1007/978-981-97-2253-2_21"
  pdf: "https://arxiv.org/pdf/2403.06292v1"
  bibtex: ticod
  summary: "In several real-world scenarios like autonomous navigation and mobility, to obtain a better visual understanding of the surroundings, image captioning and object detection play a crucial role. This work introduces a novel multitask learning framework that combines image captioning and object detection intoa joint model. We propose TICOD, Transformer-based Image Captioning andObject Detection model for jointly training both tasks by combining the losses obtained from image captioning and object detection networks. By leveraging joint training, the model benefits from the complementary information shared AQ1between the two tasks, leading to improved performance for image captioning.Our approach utilizes a transformer-based architecture that enables end-to-end AQ2network integration for image captioning and object detection and performs bothtasks jointly. We evaluate the effectiveness of our approach through comprehensive experiments on the MS-COCO dataset. Our model outperforms the baselinesfrom image captioning literature by achieving a 3.65% improvement in BERTScore."
  
- key: "charspan"
  authors: "Kaushal Maurya, Rahul Kejriwal, Maunendra Desarkar, Anoop Kunchukuttan"
  title: "CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages"
  journal: "18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024)"
  abstract: "We address the task of machine translation(MT) from extremely low-resource language(ELRL) to English by leveraging cross-lingualtransfer from closely-related high-resourcelanguage (HRL). The development of an MTsystem for ELRL is challenging because theselanguages typically lack parallel corpora andmonolingual corpora, and their representationsare absent from large multilingual languagemodels. Many ELRLs share lexical similaritieswith some HRLs, which presents a novelmodeling opportunity. However, existingsubword-based neural MT models do notexplicitly harness this lexical similarity, as theyonly implicitly align HRL and ELRL latentembedding space. To overcome this limitation,we propose a novel, CHARSPAN, approachbased on character-span noise augmentationinto the training data of HRL. This serves asa regularization technique, making the modelmore robust to lexical divergences betweenthe HRL and ELRL, thus facilitating effectivecross-lingual transfer. Our method significantlyoutperformed strong baselines in zero-shotsettings on closely related HRL and ELRL pairsfrom three diverse language families, emergingas the state-of-the-art model for ELRLs."
  year: 2024
  month: 3
  highlight: 0
  img: "CharSpan-EACL-2024.png"
  url: "https://arxiv.org/abs/2305.05214"
  pdf: "https://aclanthology.org/2024.eacl-short.26.pdf"
  bibtex: charspan
  summary: "We address the task of machine translation(MT) from extremely low-resource language(ELRL) to English by leveraging cross-lingualtransfer from closely-related high-resourcelanguage (HRL). The development of an MTsystem for ELRL is challenging because theselanguages typically lack parallel corpora andmonolingual corpora, and their representationsare absent from large multilingual languagemodels. Many ELRLs share lexical similaritieswith some HRLs, which presents a novelmodeling opportunity. However, existingsubword-based neural MT models do notexplicitly harness this lexical similarity, as theyonly implicitly align HRL and ELRL latentembedding space. To overcome this limitation,we propose a novel, CHARSPAN, approachbased on character-span noise augmentationinto the training data of HRL. This serves asa regularization technique, making the modelmore robust to lexical divergences betweenthe HRL and ELRL, thus facilitating effectivecross-lingual transfer. Our method significantlyoutperformed strong baselines in zero-shotsettings on closely related HRL and ELRL pairsfrom three diverse language families, emergingas the state-of-the-art model for ELRLs."

# Year 2023

- key: "select-noise"
  authors: "Maharaj Brahma, Kaushal Kumar Maurya, Maunendra Sankar Desarkar"
  title: "SelectNoise: Unsupervised Noise Injection to Enable Zero-shot Machine Translation for Extremely Low-resource languages"
  j0ournal: "Findings of the Association for Computational Linguistics: EMNLP 2023"
  abstract: "In this work, we focus on the task of machine translation (MT) from extremely low-resource language (ELRLs) to English. The unavailability of parallel data, lack of representation from large multilingual pre-trained models, and limited monolingual data hinder the development of MT systems for ELRLs. However, many ELRLs often share lexical similarities with high-resource languages (HRLs) due to factors such as dialectical variations, geographical proximity, and language structure. We utilize this property to improve cross-lingual signals from closely related HRL to enable MT for ELRLs. Specifically, we propose a novel unsupervised approach, SelectNoise, based on selective candidate extraction and noise injection to generate noisy HRLs training data. The noise injection acts as a regularizer, and the model trained with noisy data learns to handle lexical variations such as spelling, grammar, and vocabulary changes, leading to improved cross-lingual transfer to ELRLs. The selective candidates are extracted using BPE merge operations and edit operations, and noise injection is performed using greedy, top-p, and top-k sampling strategies. We evaluate the proposed model on 12 ELRLs from the FLORES-200 benchmark in a zero-shot setting across two language families. The proposed model outperformed all the strong baselines, demonstrating its efficacy. It has comparable performance with the supervised noise injection model."
  year: 2023
  month: 10
  highlight: 1
  url: "https://aclanthology.org/2023.findings-emnlp.109/"
  pdf: "https://aclanthology.org/2023.findings-emnlp.109.pdf"
  code: "https://github.com/maharajbrahma/selectnoise/"
  img: "select-noise-2023.png"
  bibtex: select-noise
  summary: "In this work, we focus on the task of machine translation (MT) from extremely low-resource language (ELRLs) to English. The unavailability of parallel data, lack of representation from large multilingual pre-trained models, and limited monolingual data hinder the development of MT systems for ELRLs. However, many ELRLs often share lexical similarities with high-resource languages (HRLs) due to factors such as dialectical variations, geographical proximity, and language structure. We utilize this property to improve cross-lingual signals from closely related HRL to enable MT for ELRLs. Specifically, we propose a novel unsupervised approach, SelectNoise, based on selective candidate extraction and noise injection to generate noisy HRLs training data. The noise injection acts as a regularizer, and the model trained with noisy data learns to handle lexical variations such as spelling, grammar, and vocabulary changes, leading to improved cross-lingual transfer to ELRLs. The selective candidates are extracted using BPE merge operations and edit operations, and noise injection is performed using greedy, top-p, and top-k sampling strategies. We evaluate the proposed model on 12 ELRLs from the FLORES-200 benchmark in a zero-shot setting across two language families. The proposed model outperformed all the strong baselines, demonstrating its efficacy. It has comparable performance with the supervised noise injection model."

- key: "limited-supervision-nlg-big-picture-workshop"
  authors: "Kaushal Kumar Maurya and Maunendra Sankar Desarkar"
  title: "Towards Low-resource Language Generation with Limited Supervision"
  journal: "Proceedings of the Big Picture Workshop, Association for Computational Linguistics"
  abstract: "We present a research narrative aimed at enabling language technology for multiple natural language generation (NLG) tasks in low-resource languages (LRLs). With approximately 7,000 languages spoken globally, many lack the resources required for model training. NLG applications for LRLs present two additional key challenges: (i) The training is more pronounced, and (ii) Zero-shot modeling is a viable research direction for scalability; however, generating zero-shot well-formed text in target LRLs is challenging. Addressing these concerns, this narrative introduces three promising research explorations that serve as a step toward enabling language technology for many LRLs. These approaches make effective use of transfer learning and limited supervision techniques for modeling. Evaluations were conducted mostly in the zero-shot setting, enabling scalability. This research narrative is an ongoing doctoral thesis."
  year: 2023
  month: 10
  highlight: 0
  url: "https://aclanthology.org/2023.bigpicture-1.7/"
  pdf: "https://aclanthology.org/2023.bigpicture-1.7.pdf"
  bibtex: limited-supervision-nlg-big-picture-workshop
  summary: "We present a research narrative aimed at enabling language technology for multiple natural language generation (NLG) tasks in low-resource languages (LRLs). With approximately 7,000 languages spoken globally, many lack the resources required for model training. NLG applications for LRLs present two additional key challenges: (i) The training is more pronounced, and (ii) Zero-shot modeling is a viable research direction for scalability; however, generating zero-shot well-formed text in target LRLs is challenging. Addressing these concerns, this narrative introduces three promising research explorations that serve as a step toward enabling language technology for many LRLs. These approaches make effective use of transfer learning and limited supervision techniques for modeling. Evaluations were conducted mostly in the zero-shot setting, enabling scalability. This research narrative is an ongoing doctoral thesis."

- key: "trie-nlg"
  authors: "Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Manish Gupta, Puneet Agrawal"
  title: "Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes"
  journal: "Journal track at European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2023)"
  abstract: "Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. This motivates us to propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model by augmenting the prefix with rich context comprising of recent session queries and top trie completions. This simple modeling approach overcomes the limitations of trie-based and NLG-based approaches and leads to state-of-the-art performance. We evaluate the Trie-NLG model using two large QAC datasets. On average, our model achieves huge ∼57% and ∼14% boost in MRR over the popular trie-based lookup and the strong BART-based baseline methods, respectively."
  year: 2023
  month: 7
  highlight: 1
  url: "https://dl.acm.org/doi/10.1007/s10618-023-00966-0"
  code: "https://github.com/kaushal0494/Trie-NLG"
  img: "Trie_QAC3.png"
  bibtex: trie-nlg
  summary: "Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. We propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model by augmenting the prefix with rich context comprising of recent session queries and top trie completions. This simple modeling approach overcomes the limitations of trie-based and NLG-based approaches and leads to state-of-the-art performance. We evaluate the Trie-NLG model using two large QAC datasets. On average, our model achieves huge ∼57% and ∼14% boost in MRR over the popular trie-based lookup and the strong BART-based baseline methods, respectively."

- key: "text-style-transfer"
  authors: "Sharan Narasimhan, Pooja Shekar,  Suvodip Dey, Maunendra Sankar Desarkar"
  title: "On Text Style Transfer via Style-Aware Masked Language Models"
  journal: "16th International Natural Language Generation Conference (INLG 2023)"
  abstract: "Text Style Transfer (TST) involves transforming a source sentence with a given style label to an output with another target style meanwhile preserving content and fluency. We look at a fill-in-the-blanks approach (also referred to as prototype editing), where the source sentence is stripped off all style-containing words and filled in with suitable words. This closely resembles a Masked Language Model (MLM) objective, with the added initial step of masking only relevant style words rather than BERT's random masking. We show this simple MLM, trained to reconstruct style-masked sentences back into their original style, can even transfer style by making this MLM Style-Aware. This simply involves appending the source sentence with a target style special token. The Style-Aware MLM (SA-MLM), now also accounts for the direction of style transfer and enables style transfer by simply manipulating these special tokens. To learn this n-word to n-word style reconstruction task, we use a single transformer encoder block with 8 heads, 2 layers and no auto-regressive decoder, making it non-generational. We empirically show that this lightweight encoder trained on a simple reconstruction task compares with elaborately engineered state-of-the-art TST models for even complex styles like Discourse or flow of logic, i.e. Contradiction to Entailment and vice-versa. Additionally, we introduce a more accurate attention-based style-masking step and a novel attention-surplus method to determine the position of masks from any arbitrary attribution model in O(1) time. Finally, we show that the SA-MLM arises naturally by considering a probabilistic framework for style transfer."
  year: 2023
  month: 7
  highlight: 0
  url: "https://aclanthology.org/2023.inlg-main.25/"
  pdf: "https://aclanthology.org/2023.inlg-main.25.pdf"
  code: "https://github.com/sharan21/Style-Masked-Language-Model"
  bibtex: text-style-transfer
  summary: "Text Style Transfer (TST) involves transforming a source sentence with a given style label to an output with another target style meanwhile preserving content and fluency. We look at a fill-in-the-blanks approach (also referred to as prototype editing), where the source sentence is stripped off all style-containing words and filled in with suitable words. We proposed a simple MLM, trained to reconstruct style-masked sentences back into their original style, can even transfer style by making this MLM Style-Aware. We empirically show that this lightweight encoder trained on a simple reconstruction task compares with elaborately engineered state-of-the-art TST models for even complex styles like Discourse or flow of logic. Additionally, we introduce a more accurate attention-based style-masking step and a novel attention-surplus method to determine the position of masks from any arbitrary attribution model in O(1) time. Finally, we show that the SA-MLM arises naturally by considering a probabilistic framework for style transfer."

- key: "dial-m"
  authors: "Suvodip Dey and Maunendra Sankar Desarkar"
  title: "Dial-M: A Masking-based Framework for Dialogue Evaluation"
  journal: "24th Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL 2023)<br><br><i> Nominated for Best Paper Award</i>"
  abstract: "In dialogue systems, automatically evaluating machine-generated responses is critical and challenging. Despite the tremendous progress in dialogue generation research, its evaluation heavily depends on human judgments. The standard word-overlapping based evaluation metrics are ineffective for dialogues. As a result, most of the recently proposed metrics are model-based and reference-free, which learn to score different aspects of a conversation. However, understanding each aspect requires a separate model, which makes them computationally expensive. To this end, we propose Dial-M, a Masking-based reference-free framework for Dialogue evaluation. The main idea is to mask the keywords of the current utterance and predict them, given the dialogue history and various conditions (like knowledge, persona, etc.), thereby making the evaluation framework simple and easily extensible for multiple datasets. Regardless of its simplicity, Dial-M achieves comparable performance to state-of-the-art metrics on several dialogue evaluation datasets. We also discuss the interpretability of our proposed metric along with error analysis."
  year: 2023
  month: 7
  highlight: 1
  url: "https://aclanthology.org/2023.sigdial-1.7"
  pdf: "https://aclanthology.org/2023.sigdial-1.7.pdf"
  code: "https://github.com/SuvodipDey/Dial-M"
  img: "dial-m_pic.png"
  bibtex: dial-m
  summary: "We propose Dial-M, a Masking-based reference-free framework for Dialogue evaluation. The main idea is to mask the keywords of the current utterance and predict them, given the dialogue history and various conditions (like knowledge, persona, etc.), thereby making the evaluation framework simple and easily extensible for multiple datasets. Regardless of its simplicity, Dial-M achieves comparable performance to state-of-the-art metrics on several dialogue evaluation datasets. We also discuss the interpretability of our proposed metric along with error analysis."

- key: "cl_hyperhawkes"
  authors: "Manisha Dubey, Srijith P. K., Maunendra Sankar Desarkar"
  title: "Time-to-Event Modeling with Hypernetwork based Hawkes Process"
  journal: "29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD 2023)"
  abstract: "Many real-world applications are associated with collection of events with timestamps, known as time-to-event data. Earthquake occurrences, social networks, and user activity logs can be represented as a sequence of discrete events observed in continuous time. Temporal point process serves as an essential tool for modeling such time-to-event data in continuous time space. Despite having massive amounts of event sequence data from various domains like social media, healthcare etc., real world application of temporal point process faces two major challenges: 1) it is not generalizable to predict events from unseen event sequences in dynamic environment 2) they are not capable of thriving in continually evolving environment with minimal supervision while retaining previously learnt knowledge. To tackle these issues, we propose HyperHawkes, a hypernetwork based temporal point process framework which is capable of modeling time of event occurrence for unseen sequences and consequently, zero-shot learning for time-to-event modeling. We also develop a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting. HyperHawkes augments the temporal point process with zero-shot modeling and continual learning capabilities. We demonstrate the application of the proposed framework through our experiments on real-world datasets. Our results show the efficacy of the proposed approach in terms of predicting future events under zero-shot regime for unseen event sequences. We also show that the proposed model is able to learn the time-to-event sequences continually while retaining information from previous event sequences, mitigating catastrophic forgetting in neural temporal point process."
  year: 2023
  month: 8
  url: "https://dl.acm.org/doi/10.1145/3580305.3599912"
  pdf: "https://dl.acm.org/doi/pdf/10.1145/3580305.3599912"
  cite: ""
  highlight: 0
  img: "cl_hyperhawkes.png"
  summary: ""
  bibtex: cl_hyperhawkes

- key: "divhsk"
  authors: "Venkatesh E, Kaushal Kumar Maurya, Deepak Kumar and Maunendra Sankar Desarkar"
  title: "DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection"
  journal: "61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)"
  abstract: "Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article, but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Towards this, we propose a novel model called DIVHSK. It has two components: KEYSELECT for selecting the important keywords, and SEQGEN, for finally generating the multiple diverse headlines. In KEYSELECT, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the mostattentive theme and general keywords from the source article. Then, cluster-specific keyword sets guide the SEQGEN, a pre-trained encoderdecoder model, to generate diverse yet semantically similar headlines. The proposed model consistently outperformed existing literature and our strong baselines and emerged as a stateof-the-art model. Additionally, We have also created a high-quality multi-reference headline dataset from news articles"
  year: 2023
  month: 07
  url: "https://aclanthology.org/2023.findings-acl.118/"
  pdf: "https://aclanthology.org/2023.findings-acl.118.pdf"
  cite: "Venkatesh E, Kaushal Maurya, Deepak Kumar, and Maunendra Sankar Desarkar. 2023. DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1879–1891, Toronto, Canada. Association for Computational Linguistics."
  code: "https://github.com/kaushal0494/DivHSK" 
  highlight: 1
  img: "divhsk_model_arch.png"
  # video: ""
  summary: "Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Toward this, we propose a novel model called DIVHSK. It has two components:KEYSELECT for selecting the important keywords, and SEQGEN, for finally generating the multiple diverse headlines. In KEYSELECT, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the most-attentive theme and general keywords from the source article. Then, cluster-specific keyword sets guide the SEQGEN, a pre-trained encoder-decoder model, to generate diverse yet semantically similar headlines. The proposed model consistently outperformed existing literature and our strong baselines and emerged as a state-of-the-art model. We have also created a high-quality multi-reference headline dataset from news articles."
  bibtex: divhsk

- key: "vta"
  authors: "Arkadipta De, Maunendra Sankar Desarkar, and Asif Ekbal"
  title: "Towards Improvement of Grounded Cross-Lingual Natural Language Inference with VisioTextual Attention"
  journal: "Natural Language Processing (Elsevier)"
  abstract: "Natural Language Inference (NLI) has been one of the fundamental tasks in Natural Language Processing (NLP). Recognizing Textual Entailment (RTE) between the two pieces of text is a crucial problem. It adds further challenges when it involves two languages, i.e., in the cross-lingual scenario. In this paper, we propose VisioTextual-Attention (VTA) — an effective visual–textual coattention mechanism for multi-modal crosslingual NLI. Through our research, we show that instead of using only linguistic input features, introducing visual features supporting the textual inputs improves the performance of NLI models if an effective cross-modal attention mechanism is carefully constructed. We perform several experiments on a standard cross-lingual textual entailment dataset in Hindi–English language pairs and show that the addition of visual information to the dataset along with our proposed VisioTextual Attention (VTA) enhances performance and surpasses the current state-of-the-art by 4.5%. Through monolingual experiments, we also show that the proposed VTA mechanism surpasses monolingual state-of-the-art by a margin of 2.89%. We argue that our VTA mechanism is model agnostic and can be used with other deep learning-based architectures for grounded cross-lingual NLI."
  year: 2023
  month: 08
  url: "https://www.sciencedirect.com/science/article/pii/S2949719123000201"
  pdf: "https://pdf.sciencedirectassets.com/782702/1-s2.0-S2949719123X0003X/1-s2.0-S2949719123000201/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDjiEH5Z0dS%2Fj1i%2BxvE%2Ft5LBmE1ENYhH4ow8kj9e%2BBOaQIgQUSZrR%2B%2Ft8f%2FA6phyddv98lb4hMipL1C8GT8kTp9RxgquwUIk%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDNC91mlZ87jLF70onCqPBbWZwRC18c%2Bwj3hYfc43bQ0AQgKt3mG34VsauFEJniQrZ4T4jUUylOa6WsxfZwc7UO%2B70Fxl6W6Cg5tr2iUdrHXDCPiCrj8yHiPbZKuEbb7bYvY7OW8v9%2FsBfQrkItNPlzoDp6RCqsyvy4Q%2FfQBBYyuNPqiBipK11V76rcFyN1Abm0IKFAaKvWj3XTkTAXMxHPzKvfN9Lr5yLRrG%2BwkBm5HwaWkHTj5XpQq8D2B%2B77IUlcbsoOS5Qh9lCXC%2By0RMsFct1m%2BEBR6QTNsuSA1kC8LenFpUYFbAow6SHg3EkTNlmgHheB4PLmUnsJvTkb%2FQ0eyStGwZuV4aAMlb8pofIdSo4G9%2Fc4aXpdb2RAvfMNp3XQEl%2BDOUSFbdgQBLhwgFXPVxLrQXnVa%2FOnBkajtKw5AQ4%2F8pWAOGa5I76eHyYFYGIMpZXHCs7tT3Nm00HDKqeeORsQCK0YO2YNAtctYs3ydZtxe5LsXGPuOfcHJH6hlhHaPSZy18ppoaLLvjkAL0S7Pafu7rCA3rJ%2FTCnXzBlP6LRc9d%2BdfBhL9elZ5%2FpseQ8VyEdG%2FXzz5Nef6Qfvo1zyfMwon%2FD7MZ13J3IHvawKY3fm9PtEi2XwW2pWKeZTJRgFXO9vX4z2ApRPER88McQuTWtYEh9qnGkmSYiEPyVf3%2B6R57gwGS%2Ft65Tz%2FRRfNh1O4XY1D%2BkoV0tUp6KvLnhdpPy2CRVwqXJFuMvUxpjrJ1wMjSXKnAA4AEGJMUn8BtI5HVh%2FepUpYRThCNcDd4w4V1Reo8ZIFbesi%2FQjg7npuwT%2BBp%2Fe5TcLHbm%2BGnumkGlb6OXMwcgsJKDUXC1UcSLREQik8q8c%2BdU5kcB8UEmWzdA%2BkXZHB%2BmoBua0k246gwprHiwQY6sQHj6OleUpPWYNW58w%2Bbw1GO9wtJM8TisQndk5m79BkBoYHTkk%2FIYeKuct9qMN2fFld2zvZAzQHXgg9whi%2FJUMiId2fuPUCLbjEux4Yas5q4s0j6r9SHWWYQtPQmfrhGCKYFVPcKJHj9b9DEn%2FVRrJlWs2hUpCGu4OZmhVSCOiYo%2BMbCN2JB6XMijERWRAsupxuKc8LNUUrWw4VJwRvH9J1pvnaG6s99m76jhRIsaxPHf%2FE%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250529T174619Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7VDOOTEW%2F20250529%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=eed499eb99fc51ba229e75b5ce220d62173babc73c883628dab6bfbcb44d345b&hash=b02db3f58191bdabed2d0fff57d7660f3b7071a51026fab2446395e7e57e3cb9&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S2949719123000201&tid=spdf-a94ad33c-ca53-4fd4-9dc2-95b4235be9e8&sid=1b380ad850dfe64f006ad0e9fa857c99cb79gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=130f5754575806035507&rr=9477cf7c9f9a9a9b&cc=in"
  cite: ""
  highlight: 0
  img: "vta-ex.png"
  summary: ""
  bibtex: vta

# Year 2022

- key: "gnom"
  authors: "Samujjwal Ghosh, Subhadeep Maji, Maunendra Sankar Desarkar"
  title: "GNoM: Graph Neural Network Enhanced Language Models for Disaster Related Multilingual Text Classification"
  journal: "WebSci 2022: 14th ACM Web Science Conference 2022"
  abstract: "Online social media works as a source of various valuable and actionable information during disasters. These information might be available in multiple languages due to the nature of user generated content. An effective system to automatically identify and categorize these actionable information should be capable to handle multiple languages and under limited supervision. However, existing works mostly focus on English language only with the assumption that sufficient labeled data is available. To overcome these challenges, we propose a multilingual disaster related text classification system which is capable to work undervmonolingual, cross-lingual and multilingual lingual scenarios and under limited supervision. Our end-to-end trainable framework combines the versatility of graph neural networks, by applying over the corpus, with the power of transformer based large language models, over examples, with the help of cross-attention between the two. We evaluate our framework over total nine English, Non-English and monolingual datasets invmonolingual, cross-lingual and multilingual lingual classification scenarios. Our framework outperforms state-of-the-art models in disaster domain and multilingual BERT baseline in terms of Weighted F1 score. We also show the generalizability of the proposed model under limited supervision."
  year: 2022
  month: 06
  url: https://dl.acm.org/doi/abs/10.1145/3501247.3531561
  pdf: https://dl.acm.org/doi/pdf/10.1145/3501247.3531561
  cite: "Samujjwal Ghosh, Subhadeep Maji, and Maunendra Sankar Desarkar. 2022. GNoM: Graph Neural Network Enhanced Language Models for Disaster Related Multilingual Text Classification. In 14th ACM Web Science Conference 2022 (WebSci '22). Association for Computing Machinery, New York, NY, USA, 55–65. https://doi.org/10.1145/3501247.3531561"
  code: 
  highlight: 0
  bibtex: gnom
  img: "gnom.png"
  video: https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3501247.3531561&file=WS22_S1_65.mp4
  summary: "We propose a multilingual disaster related text classification system which is capable to work undervmonolingual, cross-lingual and multilingual lingual scenarios and under limited supervision. Our end-to-end trainable framework combines the versatility of graph neural networks, by applying over the corpus, with the power of transformer based large language models, over examples, with the help of cross-attention between the two. We evaluate our framework over total nine English, Non-English and monolingual datasets invmonolingual, cross-lingual and multilingual lingual classification scenarios."


- key: "hasoc2021"
  authors: "Aditi Bagora, Kamal Shrestha, Kaushal Maurya, Maunendra Sankar Desarkar"
  title: "Hostility Detection in Online Hindi-English Code-Mixed Conversations"
  journal: "WebSci 2022: 14th ACM Web Science Conference 2022"
  abstract: "With the rise in accessibility and popularity of various social media platforms, people have started expressing and communicating their ideas, opinions, and interests online. While these platforms are active sources of entertainment and idea-sharing, they also attract hostile and offensive content equally. Identification of hostile posts is an essential and challenging task. In particular, Hindi-English Code-Mixed online posts of conversational nature (which have a hierarchy of posts, comments, and replies) have escalated the challenges. There are two major challenges: (1) the complex structure of Code-Mixed text and (2) filtering the relevant previous context for a given utterance. To overcome these challenges, in this paper, we propose a novel hierarchical neural network architecture to identify hostile posts/comments/replies in online Hindi-English Code-Mixed conversations. We leverage large multilingual pre-trained (mLPT) models like mBERT, XLMR, and MuRIL. The mLPT models provide a rich representation of code-mix text and hierarchical modeling leads to a natural abstraction and selection of the relevant context. The propose model consistently outperformed all the baselines and emerged as a state-of-the-art performing model. We conducted multiple analyses and ablation studies to prove the robustness of the proposed model."
  year: 2022
  month: 06
  url: https://dl.acm.org/doi/10.1145/3501247.3531579
  pdf: https://dl.acm.org/doi/pdf/10.1145/3501247.3531579
  cite: "Aditi Bagora, Kamal Shrestha, Kaushal Maurya, and Maunendra Sankar Desarkar. 2022. Hostility Detection in Online Hindi-English Code-Mixed Conversations. In 14th ACM Web Science Conference 2022 (WebSci '22). Association for Computing Machinery, New York, NY, USA, 390–400. https://doi.org/10.1145/3501247.3531579"
  code: https://github.com/AditiBagora/Hasoc2021CodeMix
  highlight: 0
  bibtex: hasoc2021
  img: "hasoc2021.png"
  video: https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3501247.3531579&file=WS22_S7_114.mp4
  summary: "We propose a novel hierarchical neural network architecture to identify hostile posts/comments/replies in online Hindi-English Code-Mixed conversations. We leverage large multilingual pretrained (mLPT) models like mBERT, XLMR, and MuRIL. The mLPT models provide a rich representation of code-mix text and hierarchical modeling leads to a natural abstraction and selection of the relevant context. The propose model consistently outperformed all the baselines and emerged as a state-of-the-art performing model. We conducted multiple analyses and ablation studies to prove the robustness of the proposed model."


- key: "eval-dst-performance"
  authors: "Suvodip Dey, Ramamohan Kummara, Maunendra Sankar Desarkar"
  title: "Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances"
  journal: "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"
  abstract: "Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance."
  year: 2022
  month: 5
  url: https://aclanthology.org/2022.acl-short.35/
  pdf: https://aclanthology.org/2022.acl-short.35.pdf
  cite: "Suvodip Dey, Ramamohan Kummara, and Maunendra Desarkar. 2022. Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 318–324, Dublin, Ireland. Association for Computational Linguistics."
  code: https://github.com/suvodipdey/fga
  highlight: 1
  img: "eval-dst-performance-new.png"
  summary: "Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the groundtruth dialogue state exactly matches the prediction. We propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance."
  bibtex: eval-dst-performance

- key: "time-event-modeling"
  authors: "Manisha Dubey, PK Srijith, Maunendra Sankar Desarkar"
  title: "Continual Learning for Time-to-Event Modeling"
  journal: "Continual Lifelong Learning Workshop at ACML 2022"
  abstract: "Temporal point process serves as an essential tool for modeling time-to-event data in continuous time space. Despite having massive amounts of event sequence data from various domains like social media, healthcare etc., real world application of temporal point process are not capable of thriving in continually evolving environment with minimal supervision while retaining previously learnt knowledge. To tackle this, we propose HyperHawkes, a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting. We demonstrate the application of the proposed framework through our experiments on two real-world datasets."
  year: 2022
  month: 10
  url: https://openreview.net/forum?id=1OHWaKZOub
  pdf: https://openreview.net/pdf?id=1OHWaKZOub
  cite: "Dubey, M., Srijith, P. K., & Desarkar, M. S. (2022). Continual Learning for Time-to-Event Modeling. In Continual Lifelong Learning Workshop at ACML 2022."
  code: 
  highlight: 0
  img: "time-event-modeling.png"
  video: 
  bibtex: time-event-modeling
  summary: "We propose HyperHawkes, a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting."

- key: "supervised-graph-contrastive"
  authors: "Samujjwal Ghosh, Subhadeep Maji, Maunendra Sankar Desarkar"
  title: "Supervised Graph Contrastive Pretraining for Text Classification"
  journal: "In Proceedings of ACM SAC Conference (SAC 2022)"
  abstract: "Contrastive pretraining techniques for text classification has been largely studied in an unsupervised setting. However, oftentimes labeled data from related tasks which share label semantics with current task is available. We hypothesize that using this labeled data effectively can lead to better generalization on current task. In this paper, we propose a novel way to effectively utilize labeled data from related tasks with a graph based supervised contrastive learning approach. We formulate a token-graph by extrapolating the supervised information from examples to tokens. Our formulation results in an embedding space where tokens with high/low probability of belonging to same class are near/further-away from one another. We also develop detailed theoretical insights which serve as a motivation for our method. In our experiments with 13 datasets, we show our method outperforms pretraining schemes by 2.5% and also example-level contrastive learning based formulation by 1.8% on average. In addition, we show cross-domain effectiveness of our method in a zero-shot setting by 3.91% on average. Lastly, we also demonstrate our method can be used as a noisy teacher in a knowledge distillation setting to significantly improve performance of transformer based models in low labeled data regime by 4.57% on average."
  year: 2022
  month: 12
  url: https://arxiv.org/abs/2112.11389
  pdf: https://arxiv.org/pdf/2112.11389.pdf
  cite: "Ghosh, S., Maji, S., & Desarkar, M. S. (2021). Supervised Graph Contrastive Pretraining for Text Classification. arXiv preprint arXiv:2112.11389."
  code: 
  highlight: 0
  img: "supervised-graph-contrastive.png"
  video: 
  bibtex: supervised-graph-contrastive
  summary: "We hypothesize that using this labeled data effectively can lead to better generalization on current task. In this paper, we propose a novel way to effectively utilize labeled data from related tasks with a graph based supervised contrastive learning approach. We formulate a token-graph by extrapolating the supervised information from examples to tokens. Our formulation results in an embedding space where tokens with high/low probability of belonging to same class are near/further-away from one another."


- key: "hyperhawkes"
  authors: "Manisha Dubey, PK Srijith, Maunendra Sankar Desarkar"
  title: "HyperHawkes: Hypernetwork based Neural Temporal Point Process"
  journal: "arXiv preprint arXiv:2205.02309"
  abstract: "Temporal point process serves as an essential tool for modeling time-to-event data in continuous time space. Despite having massive amounts of event sequence data from various domains like social media, healthcare etc., real world application of temporal point process faces two major challenges: 1) it is not generalizable to predict events from unseen sequences in dynamic environment 2) they are not capable of thriving in continually evolving environment with minimal supervision while retaining previously learnt knowledge. To tackle these issues, we propose \textit{HyperHawkes}, a hypernetwork based temporal point process framework which is capable of modeling time of occurrence of events for unseen sequences. Thereby, we solve the problem of zero-shot learning for time-to-event modeling. We also develop a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting. In this way, \textit{HyperHawkes} augments the temporal point process with zero-shot modeling and continual learning capabilities. We demonstrate the application of the proposed framework through our experiments on two real-world datasets. Our results show the efficacy of the proposed approach in terms of predicting future events under zero-shot regime for unseen event sequences. We also show that the proposed model is able to predict sequences continually while retaining information from previous event sequences, hence mitigating catastrophic forgetting for time-to-event data."
  year: 2022
  month: 8
  url: https://arxiv.org/abs/2210.00213
  pdf: https://arxiv.org/pdf/2210.00213.pdf
  cite: "Dubey, M., Srijith, P. K., & Desarkar, M. S. (2022). HyperHawkes: Hypernetwork based Neural Temporal Point Process. arXiv preprint arXiv:2210.00213"
  code: 
  highlight: 0
  img: "hyperhawkes.png"
  video: 
  bibtex: hyperhawkes
  summary: "we propose HyperHawkes, a hypernetwork based temporal point process framework which is capable of modeling time of occurrence of events for unseen sequences. Thereby, we solve the problem of zero-shot learning for time-to-event modeling. We also develop a hypernetwork based continually learning temporal point process for continuous modeling of time-to-event sequences with minimal forgetting. In this way, HyperHawkes augments the temporal point process with zero-shot modeling and continual learning capabilities"


- key: "graph-contrastive-pretraining"
  authors: "Samujjwal Ghosh, Subhadeep Maji, and Maunendra Sankar Desarkar"
  title: "Effective utilization of labeled data from related tasks using graph contrastive pretraining: application to disaster related text classification"
  journal: "SAC '22: Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing"
  abstract: "Contrastive pretraining techniques for text classification has been largely studied in an unsupervised setting. However, oftentimes labeled data from related past datasets which share label semantics with current task is available. We hypothesize that using this labeled data effectively can lead to better generalization on current task. In this paper, we propose a novel way to effectively utilize labeled data from related tasks with a graph based supervised contrastive learning approach. We formulate a token-graph by extrapolating the supervised information from examples to tokens. Our experiments with 8 disaster datasets show our method outperforms baselines and also example-level contrastive learning based formulation. In addition, we show cross-domain effectiveness of our method in a zero-shot setting."
  year: 2022
  month: 06
  url: https://dl.acm.org/doi/10.1145/3477314.3507194
  pdf: https://dl.acm.org/doi/pdf/10.1145/3477314.3507194
  cite: "Samujjwal Ghosh, Subhadeep Maji, and Maunendra Sankar Desarkar. 2022. Effective utilization of labeled data from related tasks using graph contrastive pretraining: application to disaster related text classification. In Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing (SAC '22). Association for Computing Machinery, New York, NY, USA, 875–878. https://doi.org/10.1145/3477314.3507194"
  code: 
  highlight: 0
  img: "graph-contrastive-pretraining.png"
  video: 
  bibtex: graph-contrastive-pretraining
  summary: "In this paper, we propose a novel way to effectively utilize labeled data from related tasks with a graph based supervised contrastive learning approach. We formulate a token-graph by extrapolating the supervised information from examples to tokens. Our experiments with 8 disaster datasets show our method outperforms baselines and also example-level contrastive learning based formulation"


- key: "unsupervised-text-style-transfer"
  authors: "Sharan Narasimhan, Suvodip Dey, Maundendra Sankar Desarkar"
  title: "Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer"
  journal: "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
  abstract: Recent studies show that auto-encoder based approaches successfully perform language generation, smooth sentence interpolation, and style transfer over unseen attributes using unlabelled datasets in a zero-shot manner. The latent space geometry of such models is organised well enough to perform on datasets where the style is 'coarse-grained' i.e. a small fraction of words alone in a sentence are enough to determine the overall style label. A recent study uses a discrete token-based perturbation approach to map 'similar' sentences ('similar' defined by low Levenshtein distance/ high word overlap) close by in latent space. This definition of 'similarity' does not look into the underlying nuances of the constituent words while mapping latent space neighbourhoods and therefore fails to recognise sentences with different style-based semantics while mapping latent neighbourhoods. We introduce EPAAEs (Embedding Perturbed Adversarial AutoEncoders) which completes this perturbation model, by adding a finely adjustable noise component on the continuous embeddings space. We empirically show that this (a) produces a better organised latent space that clusters stylistically similar sentences together, (b) performs best on a diverse set of text style transfer tasks than similar denoising-inspired baselines, and (c) is capable of fine-grained control of Style Transfer strength. We also extend the text style transfer tasks to NLI datasets and show that these more complex definitions of style are learned best by EPAAE. To the best of our knowledge, extending style transfer to NLI tasks has not been explored before.
  year: 2022
  month: 05
  url: https://aclanthology.org/2022.naacl-main.34/
  pdf: https://aclanthology.org/2022.naacl-main.34.pdf
  cite: "Narasimhan, S., Dey, S., & Desarkar, M. S. (2022). Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer. arXiv preprint arXiv:2205.02309."
  code: https://github.com/sharan21/EPAAE 
  highlight: 1
  img: "unsupervised-text-style-transfer.png"
  video: https://aclanthology.org/2022.naacl-main.34.mp4
  summary: "We introduce EPAAEs (Embedding Perturbed Adversarial AutoEncoders) which completes this perturbation model, by adding a finely adjustable noise component on the continuous embeddings space. We empirically show that this (a) produces a better organised latent space that clusters stylistically similar sentences together, (b) performs best on a diverse set of text style transfer tasks than similar denoising-inspired baselines, and (c) is capable of fine-grained control of Style Transfer strength."
  bibtex: unsupervised-text-style-transfer

- key: "maurya-desarkar-2022-meta"
  authors: "Kaushal Maurya and Maunendra Sankar Desarkar"
  title: "Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation"
  journal: "Findings of the Association for Computational Linguistics, ACL 2022"
  abstract: "Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-X$_{NLG}$) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks."
  year: 2022
  month: 5
  url: https://aclanthology.org/2022.findings-acl.24
  pdf: https://aclanthology.org/2022.findings-acl.24.pdf
  cite: "Kaushal Maurya and Maunendra Desarkar. 2022. Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 269–284, Dublin, Ireland. Association for Computational Linguistics"
  code: https://github.com/kaushal0494/Meta_XNLG 
  highlight: 1
  img: "maurya-desarkar-2022-meta.png"
  video: https://aclanthology.org/2022.findings-acl.24.mp4 
  summary: "We propose a novel meta-learning framework (called Meta-XNLG) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting."
  bibtex: maurya-desarkar-2022-meta

- key: "unspuervised-domain-gnn"
  authors: "Sammujwal Ghosh, Subhadeep Maji and Maunendra Sankar Desarkar"
  title: "Unsupervised Domain Adaptation With Global and Local Graph Neural Networks Under Limited Supervision and Its Application to Disaster Response"
  journal: "IEEE Transactions on Computational Social Systems (Volume: 10, Issue: 2, April 2023)"
  abstract: "Identification and categorization of social media posts generated during disasters are crucial to reduce the suffering of the affected people. However, the lack of labeled data is a significant bottleneck in learning an effective categorization system for a disaster. This motivates us to study the problem as unsupervised domain adaptation (UDA) between a previous disaster with labeled data (source) and a current disaster (target). However, if the amount of labeled data available is limited, it restricts the learning capabilities of the model. To handle this challenge, we use limited labeled data along with abundantly available unlabeled data, generated during a source disaster to propose a novel two-part graph neural network (GNN). The first part extracts domain-agnostic global information by constructing a token-level graph across domains and the second part preserves local instance-level semantics. In our experiments, we show that the proposed method outperforms state-of-the-art techniques by 2.74% weighted F1 score on average on two standard public datasets in the area of disaster management. We also report experimental results for granular actionable multilabel classification datasets in disaster domain for the first time, on which we outperform BERT by 3.00% on average w.r.t. weighted F1. Additionally, we show that our approach can retain performance when minimal labeled data are available."
  year: 2022
  month: 03
  url: https://ieeexplore.ieee.org/document/9744724
  pdf: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9744724
  cite: "S. Ghosh, S. Maji and M. S. Desarkar, 'Unsupervised Domain Adaptation With Global and Local Graph Neural Networks Under Limited Supervision and Its Application to Disaster Response,' in IEEE Transactions on Computational Social Systems, vol. 10, no. 2, pp. 551-562, April 2023, doi: 10.1109/TCSS.2022.3159109."
  code: 
  highlight: 1
  img: unspuervised-domain-gnn.png
  video: 
  summary: "In our experiments, we show that the proposed method outperforms state-of-the-art techniques by 2.74% weighted F1 score on average on two standard public datasets in the area of disaster management. We also report experimental results for granular actionable multilabel classification datasets in disaster domain for the first time, on which we outperform BERT by 3.00% on average w.r.t. weighted F1."
  bibtex: unspuervised-domain-gnn

# Year 2021

- key: "hostility"
  authors: "Arkadipta De, Venkatesh E, Kaushal Kumar Maurya, and Maunendra Sankar Desarkar"
  title: "Coarse and Fine-Grained Hostility Detection in Hindi Posts using Fine Tuned Multilingual Embeddings"
  journal: CONSTRAIN 2021 (Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situation)
  abstract: "Due to the wide adoption of social media platforms like Facebook, Twitter, etc., there is an emerging need of detecting online posts that can go against the community acceptance standards. The hostility detection task has been well explored for resource-rich languages like English, but is unexplored for resource-constrained languages like Hindi due to the unavailability of large suitable data. We view this hostility detection as a multi-label multi-class classification problem. We propose an effective neural network-based technique for hostility detection in Hindi posts. We leverage pre-trained multilingual Bidirectional Encoder Representations of Transformer (mBERT) to obtain the contextual representations of Hindi posts. We have performed extensive experiments including different pre-processing techniques, pre-trained models, neural architectures, hybrid strategies, etc. Our best performing neural classifier model includes One-vs-the-Rest approach where we obtained 92.60%, 81.14%, 69.59%, 75.29% and 73.01% F1 scores for hostile, fake, hate, offensive, and defamation labels respectively. The proposed model (https://​github.​com/​Arko98/​Hostility-Detection-in-Hindi-Constraint-2021) outperformed the existing baseline models and emerged as the state-of-the-art model for detecting hostility in the Hindi posts."
  year: 2021
  month: 
  url: https://www.springerprofessional.de/en/coarse-and-fine-grained-hostility-detection-in-hindi-posts-using/19047892
  pdf: "https://arxiv.org/pdf/2101.04998.pdf"
  cite: "De, A., Elangovan, V., Maurya, K. K., & Desarkar, M. S. (2021). Coarse and fine-grained hostility detection in Hindi posts using fine tuned multilingual embeddings. In Combating Online Hostile Posts in Regional Languages during Emergency Situation: First International Workshop, CONSTRAINT 2021, Collocated with AAAI 2021, Virtual Event, February 8, 2021, Revised Selected Papers 1 (pp. 201-212). Springer International Publishing."
  code: https://github.com/Arko98/Hostility-Detection-in-Hindi-Constraint-2021
  highlight: 0
  img: "hostility.png"
  video: 
  bibtex: hostility
  summary: "We propose an effective neural network-based technique for hostility detection in Hindi posts. We leverage pre-trained multilingual Bidirectional Encoder Representations of Transformer (mBERT) to obtain the contextual representations of Hindi posts. We have performed extensive experiments including different pre-processing techniques, pre-trained models, neural architectures, hybrid strategies, etc. Our best performing neural classifier model includes One-vs-the-Rest approach where we obtained 92.60%, 81.14%, 69.59%, 75.29% and 73.01% F1 scores for hostile, fake, hate, offensive, and defamation labels respectively."


- key: "multi-view-hypergraph"
  authors: "Manisha Dubey, P.K Srijith and Maunendra Sankar Desarkar"
  title: "Multi-view Hypergraph Convolution Network for Semantic Annotation in LBSNs"
  journal: "ASONAM 2021: Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining"
  abstract: "Semantic characterization of the Point-of-Interest (POI) plays an important role for modeling location-based social networks and various related applications like POI recommendation, link prediction etc. However, semantic categories are not available for many POIs which makes this characterization difficult. Semantic annotation aims to predict such missing categories of POIs. Existing approaches learn a representation of POIs using graph neural networks to predict semantic categories. However, LBSNs involve complex and higher order mobility dynamics. These higher order relations can be captured effectively by employing hypergraphs. Moreover, visits to POIs can be attributed to various reasons like temporal characteristics, spatial context etc. Hence, we propose a Multi-view Hypergraph Convolution Network (Multi-HGCN) where we learn POI representations by considering multiple hypergraphs across multiple views of the data. We build a comprehensive model to learn the POI representation capturing temporal, spatial and trajectorybased patterns among POIs by employing hypergraphs. We use hypergraph convolution to learn better POI representation by using spectral properties of hypergraph. Experiments conducted on three real-world datasets show that the proposed approach outperforms the state-of-the-art approaches."
  year: 2021
  month: 11
  url: https://dl.acm.org/doi/10.1145/3487351.3488341
  pdf: https://dl.acm.org/doi/pdf/10.1145/3487351.3488341
  cite: "Manisha Dubey, P. K. Srijith, and Maunendra Sankar Desarkar. 2022. Multi-view hypergraph convolution network for semantic annotation in LBSNs. In Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM '21). Association for Computing Machinery, New York, NY, USA, 219–227. https://doi.org/10.1145/3487351.3488341"
  code: 
  highlight: 0
  img: "multi-view-hypergraph.png" 
  video: 
  bibtex: multi-view-hypergraph
  summary: "We propose a Multi-view Hypergraph Convolution Network (Multi-HGCN) where we learn POI representations by considering multiple hypergraphs across multiple views of the data. We build a comprehensive model to learn the POI representation capturing temporal, spatial and trajectory-based patterns among POIs by employing hypergraphs. We use hypergraph convolution to learn better POI representation by using spectral properties of hypergraph. Experiments conducted on three real-world datasets show that the proposed approach outperforms the state-of-the-art approaches."

- key: "maurya-etal-2021-zmbart"
  authors: "Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Yoshinobu Kano, and Kumari Deepshikha"
  title: "ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation"
  journal: "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
  abstract: "Despite the recent advancement in NLP research, cross-lingual transfer for natural language generation is relatively understudied. In this work, we transfer supervision from high resource language (HRL) to multiple lowresource languages (LRLs) for natural language generation (NLG). We consider four NLG tasks (text summarization, question generation, news headline generation, and distractor generation) and three syntactically diverse languages, i.e., English, Hindi, and Japanese. We propose an unsupervised crosslingual language generation framework (called ZmBART) that does not use any parallel or pseudo-parallel/back-translated data. In this framework, we further pre-train mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task using monolingual data of three languages. The objective function of the auxiliary task is close to the target tasks which enriches the multi-lingual latent representation of mBART and provides good initialization for target tasks. Then, this model is fine-tuned with task-specific supervised English data and directly evaluated with low-resource languages in the Zero-shot setting. To overcome catastrophic forgetting and spurious correlation issues, we applied freezing model component and data argumentation approaches respectively. This simple modeling approach gave us promising results. We experimented with few-shot training (with 1000 supervised data-points) which boosted the model performance further. We performed several ablations and cross-lingual transferability analysis to demonstrate the robustness of ZmBART."
  year: 2021
  month: 8
  url: https://aclanthology.org/2021.findings-acl.248/ 
  pdf: https://aclanthology.org/2021.findings-acl.248.pdf 
  cite: "Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Yoshinobu Kano, and Kumari Deepshikha. 2021. ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2804–2818, Online. Association for Computational Linguistics."
  code: "https://github.com/kaushal0494/ZmBART"
  highlight: 1
  img: "maurya-etal-2021-zmbart.png"
  video: "https://aclanthology.org/2021.findings-acl.248.mp4"
  summary: "We propose an unsupervised crosslingual language generation framework (called ZmBART) that does not use any parallel or pseudo-parallel/back-translated data. In this framework, we further pre-train mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task using monolingual data of three languages. The objective function of the auxiliary task is close to the target tasks which enriches the multi-lingual latent representation of mBART and provides good initialization for target tasks. Then, this model is fine-tuned with task-specific supervised English data and directly evaluated with low-resource languages in the Zero-shot setting."
  bibtex: maurya-etal-2021-zmbart

- key: "hidst"
  authors: "Suvodip Dey, Maunendra Sankar Desarkar"
  title: "Hi-DST: A Hierarchical Approach for Scalable and Extensible Dialogue State Tracking"
  journal: "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue"
  abstract: "Dialogue State Tracking (DST) is a sub-task of task-based dialogue systems where the user intention is tracked through a set of (domain, slot, slot-value) triplets. Existing DST models can be difficult to extend for new datasets with larger domains/slots mainly due to either of the two reasons- i) prediction of domain-slot as a pair, and ii) dependency of model parameters on the number of slots and domains. In this work, we propose to address these issues using a Hierarchical DST (Hi-DST) model. At a given turn, the model first detects a change in domain followed by domain prediction if required. Then it decides suitable action for each slot in the predicted domains and finds their value accordingly. The model parameters of Hi-DST are independent of the number of domains/slots. Due to the hierarchical modeling, it achieves O(|M|+|N|) belief state prediction for a single turn where M and N are the set of unique domains and slots respectively. We argue that the hierarchical structure helps in the model explainability and makes it easily extensible to new datasets. Experiments on the MultiWOZ dataset show that our proposed model achieves comparable joint accuracy performance to state-of-the-art DST models."
  year: 2021
  month: 07
  url: https://aclanthology.org/2021.sigdial-1.23/
  pdf: https://aclanthology.org/2021.sigdial-1.23.pdf
  cite: "Suvodip Dey and Maunendra Sankar Desarkar. 2021. Hi-DST: A Hierarchical Approach for Scalable and Extensible Dialogue State Tracking. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 218–227, Singapore and Online. Association for Computational Linguistics."
  code: https://github.com/suvodipdey/hi-dst
  highlight: 1
  img: "hidst.png"
  video: "https://www.youtube.com/watch?v=ldnP2Cn_7F0"
  summary: "In this work, we propose to address these issues using a Hierarchical DST (Hi-DST) model. At a given turn, the model first detects a change in domain followed by domain prediction if required. Then it decides suitable action for each slot in the predicted domains and finds their value accordingly. The model parameters of Hi-DST are independent of the number of domains/slots. Due to the hierarchical modeling, it achieves O(|M|+|N|) belief state prediction for a single turn where M and N are the set of unique domains and slots respectively."
  bibtex: hidst

# Year 2020

- key: "granular-classification-framework"
  authors: "Samujjwal Ghosh and Maunendra Sankar Desarkar"
  title: "Semi-Supervised Granular Classification Framework for Resource Constrained Short-texts: Towards Retrieving Situational Information During Disaster Events"
  journal: "WebSci 2020: 12th ACM Conference on Web Science"
  abstract: "During the time of disasters, lots of short-texts are generated containing crucial situational information. Proper extraction and identification of situational information might be useful for various rescue and relief operations. Few specific types of infrequent situational information might be critical. However, obtaining labels for those resource-constrained classes is challenging as well as expensive. Supervised methods pose limited usability in such scenarios. To overcome this challenge, we propose a semi-supervised learning framework which utilizes abundantly available unlabelled data by self-learning. The proposed framework improves the performance of the classifier for resource-constrained classes by selectively incorporating highly confident samples from unlabelled data for self-learning. Incremental incorporation of unlabelled data, as and when they become available, is suitable for ongoing disaster mitigation. Experiments on three disaster-related datasets show that such improvement results in overall performance increase over standard supervised approach."
  year: 2020
  month: 07
  url: https://dl.acm.org/doi/10.1145/3394231.3397892
  pdf: https://dl.acm.org/doi/pdf/10.1145/3394231.3397892
  cite: "Samujjwal Ghosh and Maunendra Sankar Desarkar. 2020. Semi-Supervised Granular Classification Framework for Resource Constrained Short-texts: Towards Retrieving Situational Information During Disaster Events. In 12th ACM Conference on Web Science (WebSci '20). Association for Computing Machinery, New York, NY, USA, 29–38. https://doi.org/10.1145/3394231.3397892"
  code: 
  highlight: 0
  bibtex: granular-classification-framework
  img: "granular-classification-framework.png"
  video: https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3394231.3397892&file=3394231.3397892.mp4
  summary: "We propose a semi-supervised learning framework which utilizes abundantly available unlabelled data by self-learning. The proposed framework improves the performance of the classifier for resource-constrained classes by selectively incorporating highly confident samples from unlabelled data for self-learning. Incremental incorporation of unlabelled data, as and when they become available, is suitable for ongoing disaster mitigation."


- key: "pericles_1468039438"
  authors: "Sreekanth Madisetty, Kaushal Kumar Maurya, Akiko Aizawa, and Maunendra Sankar Desarkar,"
  title: "A Neural Approach for Detecting Inline Mathematical Expressions from Scientific Documents"
  journal: "Wiley Expert Systems"
  abstract: "Scientific documents generally contain multiple mathematical expressions in them. Detecting inline mathematical expressions are one of the most important and challenging tasks in scientific text mining. Recent works that detect inline mathematical expressions in scientific documents have looked at the problem from an image processing perspective. There is little work that has targeted the problem from NLP perspective. Towards this, we define a few features and applied Conditional Random Fields (CRF) to detect inline mathematical expressions in scientific documents. Apart from this feature based approach, we also propose a hybrid algorithm that combines Bidirectional Long Short Term Memory networks (Bi-LSTM) and feature-based approach for this task. Experimental results suggest that this proposed hybrid method outperforms several baselines in the literature and also individual methods in the hybrid approach."
  year: 2020
  month: 05
  url: https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12576
  pdf: https://doi.org/10.1111/exsy.12576
  cite: "Madisetty, S., Maurya, K. K., Aizawa, A., & Desarkar, M. S. (2021). A neural approach for detecting inline mathematical expressions from scientific documents. Expert Systems, 38(4), e12576."
  code: 
  highlight: 0
  # img: "pericles_1468039438.png"  
  bibtex: pericles_1468039438
  video: 
  summary: 


- key: "hapsap"
  authors: "Manisha Dubey, P.K Srijith and Maunendra Sankar Desarkar" 
  title: "HAP-SAP: Semantic Annotation in LBSNs using Latent Spatio-Temporal Hawkes Process"
  journal: "SIGSPATIAL 2020: Proceedings of the 28th International Conference on Advances in Geographic Information Systems"
  abstract: "The prevalence of location-based social networks (LBSNs) has eased the understanding of human mobility patterns. However, categories which act as semantic characterization of the location, might be missing for some check-ins and can adversely affect modelling the mobility dynamics of users. At the same time, mobility patterns provide cues on the missing semantic categories. In this paper, we simultaneously address the problem of semantic annotation of locations and location adoption dynamics of users. We propose our model HAP-SAP, a latent spatio-temporal multivariate Hawkes process, which considers latent semantic category influences, and temporal and spatial mobility patterns of users. The inferred semantic categories can supplement our model on predicting the next check-in events by users. Our experiments on real datasets demonstrate the effectiveness of the proposed model for the semantic annotation and location adoption modelling tasks."
  year: 2020
  month: 11
  url: https://dl.acm.org/doi/10.1145/3397536.3422233
  pdf: https://dl.acm.org/doi/pdf/10.1145/3397536.3422233
  cite: "Manisha Dubey, P.K. Srijith, and Maunendra Sankar Desarkar. 2020. HAP-SAP: Semantic Annotation in LBSNs using Latent Spatio-Temporal Hawkes Process. In Proceedings of the 28th International Conference on Advances in Geographic Information Systems (SIGSPATIAL '20). Association for Computing Machinery, New York, NY, USA, 377–380. https://doi.org/10.1145/3397536.3422233"
  code: 
  highlight: 0
  img: "hapsap.png"
  video: 
  bibtex: hapsap
  summary: "We propose our model HAP-SAP, a latent spatio-temporal multivariate Hawkes process, which considers latent semantic category influences, and temporal and spatial mobility patterns of users. The inferred semantic categories can supplement our model on predicting the next check-in events by users. Our experiments on real datasets demonstrate the effectiveness of the proposed model for the semantic annotation and location adoption modelling tasks."

- key: "learningtodistract"
  authors: "Kaushal Kumar Maurya and Maunendra Sankar Desarkar"
  title: "Learning to Distract: A Hierarchical Multi-Decoder Network for Automated Generation of Long Distractors for Multiple-Choice Questions for Reading Comprehension"
  journal: "CIKM 2020: Proceedings of the 29th ACM International Conference on Information & Knowledge Management"
  abstract: "The task of generating incorrect options for multiple-choice questions is termed as distractor generation problem. The task requires high cognitive skills and is extremely challenging to automate. Existing neural approaches for the task leverage encoder-decoder architecture to generate long distractors. However, in this process two critical points are ignored - firstly, many methods use Jaccard similarity over a pool of candidate distractors to sample the distractors. This often makes the generated distractors too obvious or not relevant to the question context. Secondly, some approaches did not consider the answer in the model, which caused the generated distractors to be either answer-revealing or semantically equivalent to the answer. In this paper, we propose a novel Hierarchical Multi-Decoder Network (HMD-Net) consisting of one encoder and three decoders, where each decoder generates a single distractor. To overcome the first problem mentioned above, we include multiple decoders with a dis-similarity loss in the loss function. To address the second problem, we exploit richer interaction between the article, question, and answer with a SoftSel operation and a Gated Mechanism. This enables the generation of distractors that are in context with questions but semantically not equivalent to the answers. The proposed model outperformed all the previous approaches significantly in both automatic and manual evaluations. In addition, we also consider linguistic features and BERT contextual embedding with our base model which further push the model performance."
  year: 2020
  month: 10
  url: "https://dl.acm.org/doi/abs/10.1145/3340531.3411997"
  pdf: "https://dl.acm.org/doi/pdf/10.1145/3340531.3411997"
  cite: "Kaushal Kumar Maurya and Maunendra Sankar Desarkar. 2020. Learning to Distract: A Hierarchical Multi-Decoder Network for Automated Generation of Long Distractors for Multiple-Choice Questions for Reading Comprehension. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (CIKM '20). Association for Computing Machinery, New York, NY, USA, 1115–1124. https://doi.org/10.1145/3340531.3411997"
  code: "https://github.com/kaushal0494/HMD_Network"
  highlight: 0
  img: "learningtodistract.png"
  video: 
  bibtex: learningtodistract
  summary: "In this paper, we propose a novel Hierarchical Multi-Decoder Network (HMD-Net) consisting of one encoder and three decoders, where each decoder generates a single distractor. To overcome the first problem mentioned above, we include multiple decoders with a dis-similarity loss in the loss function. To address the second problem, we exploit richer interaction between the article, question, and answer with a SoftSel operation and a Gated Mechanism."

# Year 2019

- key: "multi-context-info"
  authors: "Swapnil Dewalkar and Maunendra Sankar Desarkar"
  title: "Multi-Context Information for Word Representation Learning"
  journal: "DocEng 2019: Proceedings of the ACM Symposium on Document Engineering 2019"
  abstract: "Word embedding techniques in literature are mostly based on Bag of Words models where words that co-occur with each other are considered to be related. However, it is not necessary for similar or related words to occur in the same context window. In this paper, we propose a new approach to combine different types of resources for training word embeddings. The lexical resources used in this work are Dependency Parse Tree and WordNet. Apart from the co-occurrence information, the use of these additional resources helps us in including the semantic and syntactic information from the text in learning the word representations. The learned representations are evaluated on multiple evaluation tasks like Semantic Textual Similarity, Word Similarity. Results of the experimental analyses highlight the usefulness of the proposed methodology."
  year: 2019
  month: 9
  url: https://dl.acm.org/doi/10.1145/3342558.3345418
  pdf: https://dl.acm.org/doi/pdf/10.1145/3342558.3345418
  cite: "Swapnil Dewalkar and Maunendra Sankar Desarkar. 2019. Multi-Context Information for Word Representation Learning. In Proceedings of the ACM Symposium on Document Engineering 2019 (DocEng '19). Association for Computing Machinery, New York, NY, USA, Article 21, 1–4. https://doi.org/10.1145/3342558.3345418"
  code: 
  highlight: 0
  img: "multi-context-info.png"
  bibtex: multi-context-info
  video: 
  summary: 

- key: "vitag"
  authors: "Abhishek A. Patwardhan, Santanu Das, Sakshi Varshney, Maunendra Sankar Desarkar, Debi Prosad Dogra"
  title: "ViTag: Automatic Video Tagging Using Segmentation and Conceptual Inference"
  journal: "2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)"
  abstract: "Massive increase in multimedia data has created a need for effective organization strategy. The multimedia collection is organized based on attributes such as domain, index-terms, content description, owners, etc. Typically, index-term is a prominent attribute for effective video retrieval systems. In this paper, we present a new approach of automatic video tagging referred to as ViTag. Our analysis relies upon various image similarity metrics to automatically extract key-frames. For each key-frame, raw tags are generated by performing reverse image tagging. The final step analyzes raw tags in order to discover hidden semantic information. On a dataset of 103 videos belonging to 13 domains derived from various YouTube categories, we are able to generate tags with 65.51% accuracy. We also rank the generated tags based upon the number of proper nouns present in it. The geometric mean of Reciprocal Rank estimated over the entire collection has been found to be 0.873."
  year: 2019
  month: 9
  url: https://ieeexplore.ieee.org/document/8919469
  pdf: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8919469
  cite: "A. A. Patwardhan, S. Das, S. Varshney, M. S. Desarkar and D. P. Dogra, ViTag: Automatic Video Tagging Using Segmentation and Conceptual Inference, 2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM), Singapore, 2019, pp. 271-276, doi: 10.1109/BigMM.2019.00-12."
  code: 
  highlight: 0
  img: "vitag.png"
  video: 
  summary: 
  bibtex: vitag

# Year 2018

- key: "recsys18"
  authors: "Rohan Tondulkar, Manisha Dubey and Maunendra Sankar Desarkar"
  title: "Get me the best: predicting best answerers in community question answering sites"
  journal: "RecSys 2018: Proceedings of the 12th ACM Conference on Recommender Systems"
  abstract: "There has been a massive rise in the use of Community Question and Answering (CQA) forums to get solutions to various technical and non-technical queries. One common problem faced in CQA is the small number of experts, which leaves many questions unanswered. This paper addresses the challenging problem of predicting the best answerer for a new question and thereby recommending the best expert for the same. Although there are work in the literature that aim to find possible answerers for questions posted in CQA, very few algorithms exist for finding the best answerer whose answer will satisfy the information need of the original Poster. For finding answerers, existing approaches mostly use features based on content and tags associated with the questions. There are few approaches that additionally consider the users' history. In this paper, we propose an approach that considers a comprehensive set of features including but not limited to text representation, tag based similarity as well as multiple user-based features that target users' availability, agility as well as expertise for predicting the best answerer for a given question. We also include features that give incentives to users who answer less but more important questions over those who answer a lot of questions of less importance. A learning to rank algorithm is used to find the weight of each feature. Experiments conducted on a real dataset from Stack Exchange show the efficacy of the proposed method in terms of multiple evaluation metrics for accuracy, robustness and real time performance."
  year: 2018
  month: 09
  url: https://dl.acm.org/doi/10.1145/3240323.3240346
  pdf: https://dl.acm.org/doi/pdf/10.1145/3240323.3240346
  cite: "Rohan Tondulkar, Manisha Dubey, and Maunendra Sankar Desarkar. 2018. Get me the best: predicting best answerers in community question answering sites. In Proceedings of the 12th ACM Conference on Recommender Systems (RecSys '18). Association for Computing Machinery, New York, NY, USA, 251–259. https://doi.org/10.1145/3240323.3240346"
  code: 
  highlight: 0
  # img: "recsys18.png"
  video: 
  bibtex: recsys18
  summary: "In this paper, we propose an approach that considers a comprehensive set of features including but not limited to text representation, tag based similarity as well as multiple user-based features that target users' availability, agility as well as expertise for predicting the best answerer for a given question. We also include features that give incentives to users who answer less but more important questions over those who answer a lot of questions of less importance." 

- key: "class-specific-tfidf"
  authors: "Samujjwal Ghosh and Maunendra Sankar Desarkar"
  title: "Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters"
  journal: "WWW 2018: Companion Proceedings of the The Web Conference 2018"
  abstract: "Proper formulation of features plays an important role in short-text classification tasks as the amount of text available is very little. In literature, Term Frequency - Inverse Document Frequency (TF-IDF) is commonly used to create feature vectors for such tasks. However, TF-IDF formulation does not utilize the class information available in supervised learning. For classification problems, if it is possible to identify terms that can strongly distinguish among classes, then more weight can be given to those terms during feature construction phase. This may result in improved classifier performance with the incorporation of extra class label related information. We propose a supervised feature construction method to classify tweets, based on the actionable information that might be present, posted during different disaster scenarios. Improved classifier performance for such classification tasks can be helpful in the rescue and relief operations. We used three benchmark datasets containing tweets posted during Nepal and Italy earthquakes in 2015 and 2016 respectively. Experimental results show that the proposed method obtains better classification performance on these benchmark datasets."
  year: 2018
  month: 04
  url: https://dl.acm.org/doi/10.1145/3184558.3191621
  pdf: https://dl.acm.org/doi/pdf/10.1145/3184558.3191621
  cite: "Samujjwal Ghosh and Maunendra Sankar Desarkar. 2018. Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters. In Companion Proceedings of the The Web Conference 2018 (WWW '18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 1629–1637. https://doi.org/10.1145/3184558.3191621"
  code: 
  highlight: 0
  img: "class-specific-tfidf.png"
  video: 
  bibtex: class-specific-tfidf
  summary: "We propose a supervised feature construction method to classify tweets, based on the actionable information that might be present, posted during different disaster scenarios. Improved classifier performance for such classification tasks can be helpful in the rescue and relief operations. We used three benchmark datasets containing tweets posted during Nepal and Italy earthquakes in 2015 and 2016 respectively. "

# Year 2017

- key: "classifying-actionable-insights"
  authors: "Samujjwal Ghosh, PK Srijith, Maunendra Sankar Desarkar"
  title: "Using social media for classifying actionable insights in disaster scenario"
  journal: "International Journal of Advances in Engineering Sciences and Applied Mathematics volume 9, pages224–237"
  abstract: "Micro-blogging sites are important source of real-time situational information during disasters such as earthquakes, hurricanes, wildfires, flood etc. Such disasters cause miseries in the lives of affected people. Timely identification of steps needed to help the affected people in such situations can mitigate those miseries to a large extent. In this paper, we focus on the problem of automated classification of disaster related tweets to a set of predefined categories. Some example categories considered are resource availability, resource requirement, infrastructure damage etc. Proper annotation of the tweets with these class information can help in timely determination of the steps needed to be taken to address the concerns of the people in the affected areas. Depending on the information category, different feature sets might be useful for proper identification of posts belonging to that category. In this work, we define multiple feature sets and use them with various supervised classification algorithms from literature to study the effectiveness of our approach in annotating the tweets with their appropriate information categories."
  year: 2017
  month: 12
  url: "https://link.springer.com/article/10.1007/s12572-017-0197-2"
  pdf: https://link.springer.com/article/10.1007/s12572-017-0197-2
  cite: "Ghosh, S., Srijith, P.K. & Desarkar, M.S. Using social media for classifying actionable insights in disaster scenario. Int J Adv Eng Sci Appl Math 9, 224–237 (2017). https://doi.org/10.1007/s12572-017-0197-2"
  code: 
  highlight: 0
  img: 
  video: 
  bibtex: classifying-actionable-insights
  summary: "We focus on the problem of automated classification of disaster related tweets to a set of predefined categories. Some example categories considered are resource availability, resource requirement, infrastructure damage etc. Proper annotation of the tweets with these class information can help in timely determination of the steps needed to be taken to address the concerns of the people in the affected areas. Depending on the information category, different feature sets might be useful for proper identification of posts belonging to that category."
